@article{yin2019fourier,
  title={A fourier perspective on model robustness in computer vision},
  author={Yin, Dong and Lopes, Raphael Gontijo and Shlens, Jonathon and Cubuk, Ekin D and Gilmer, Justin},
  journal={arXiv preprint arXiv:1906.08988},
  year={2019}
}

@article{novak2018sensitivity,
  title={Sensitivity and generalization in neural networks: an empirical study},
  author={Novak, Roman and Bahri, Yasaman and Abolafia, Daniel A and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1802.08760},
  year={2018}
}

@inproceedings{tsuzuku2019structural,
  title={On the structural sensitivity of deep convolutional networks to the directions of fourier basis functions},
  author={Tsuzuku, Yusuke and Sato, Issei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={51--60},
  year={2019}
}


@inproceedings{chizat2020implicit,
	title = {Implicit {Bias} of {Gradient} {Descent} for {Wide} {Two}-layer {Neural} {Networks} {Trained} with the {Logistic} {Loss}},
	url = {http://proceedings.mlr.press/v125/chizat20a.html},
	abstract = {Neural networks trained to minimize the logistic (a.k.a. cross-entropy) loss with gradient-based methods are observed to perform well in many supervised classification tasks. Towards understanding...},
	language = {en},
	urldate = {2020-12-30},
	booktitle = {Conference on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Chizat, Lénaïc and Bach, Francis},
	month = jul,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1305--1338},
	file = {Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\4DGERV9X\\chizat20a.html:text/html;Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\HFQT8Q5V\\Chizat and Bach - 2020 - Implicit Bias of Gradient Descent for Wide Two-lay.pdf:application/pdf;Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\D5Z4IIMI\\chizat20a.html:text/html},
}

@book{Statistical_Mechanics,
  title={Statistical Mechanics},
  author={Beale, P.D.},
  isbn={9780080541716},
  url={https://books.google.ch/books?id=PIk9sF9j2oUC},
  year={1996},
  publisher={Elsevier Science}
}
@article{tomasini2022failure,
  title={Failure and success of the spectral bias prediction for Kernel Ridge Regression: the case of low-dimensional data},
  author={Tomasini, Umberto M and Sclocchi, Antonio and Wyart, Matthieu},
  journal={arXiv preprint arXiv:2202.03348},
  year={2022}
}
@article{degiuli2015theory,
  title={Theory of the jamming transition at finite temperature},
  author={Degiuli, Eric and Lerner, E and Wyart, M},
  journal={The Journal of chemical physics},
  volume={142},
  number={16},
  pages={164503},
  year={2015},
  publisher={AIP Publishing LLC}
}

@inproceedings{jacot2020implicit,
	title = {Implicit {Regularization} of {Random} {Feature} {Models}},
	url = {http://proceedings.mlr.press/v119/jacot20a.html},
	abstract = {Random Features (RF) models are used as efficient parametric approximations of kernel methods. We investigate, by means of random matrix theory, the connection between Gaussian RF models and Kernel...},
	language = {en},
	urldate = {2020-12-29},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Jacot, Arthur and Simsek, Berfin and Spadaro, Francesco and Hongler, Clement and Gabriel, Franck},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {4631--4640},
	file = {Snapshot:C\:\\Users\\leope\\Zotero\\storage\\CMIQKW5B\\jacot20a.html:text/html;Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\GKZRQTAQ\\Jacot et al. - 2020 - Implicit Regularization of Random Feature Models.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\HZFNA3AV\\jacot20a.html:text/html}
}

@article{chen2020dynamical,
	title = {A {Dynamical} {Central} {Limit} {Theorem} for {Shallow} {Neural} {Networks}},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/fc5b3186f1cf0daece964f78259b7ba0-Abstract.html},
	language = {en},
	urldate = {2020-12-30},
	journal = {Advances in Neural Information Processing Systems},
	author = {Chen, Zhengdao and Rotskoff, Grant and Bruna, Joan and Vanden-Eijnden, Eric},
	year = {2020},
	file = {Snapshot:C\:\\Users\\leope\\Zotero\\storage\\8ED33A2T\\fc5b3186f1cf0daece964f78259b7ba0-Abstract.html:text/html;Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\S3AF3PHK\\Chen et al. - 2020 - A Dynamical Central Limit Theorem for Shallow Neur.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\8YCRLAFY\\fc5b3186f1cf0daece964f78259b7ba0-Abstract.html:text/html}
}


@article{dyer2019asymptotics,
  title={Asymptotics of wide networks from feynman diagrams},
  author={Dyer, Ethan and Gur-Ari, Guy},
  journal={arXiv preprint arXiv:1909.11304},
  year={2019}
}

@article{hanin2019finite,
  title={Finite depth and width corrections to the neural tangent kernel},
  author={Hanin, Boris and Nica, Mihai},
  journal={arXiv preprint arXiv:1909.05989},
  year={2019}
}
@article{franz2019critical,
  title={Critical jammed phase of the linear perceptron},
  author={Franz, Silvio and Sclocchi, Antonio and Urbani, Pierfrancesco},
  journal={Physical Review Letters},
  volume={123},
  number={11},
  pages={115702},
  year={2019},
  publisher={APS}
}
@article{wyart2010scaling,
  title={Scaling of phononic transport with connectivity in amorphous solids},
  author={Wyart, Matthieu},
  journal={EPL (Europhysics Letters)},
  volume={89},
  number={6},
  pages={64001},
  year={2010},
  publisher={IOP Publishing}
}
@article{belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019},
  publisher={National Acad Sciences}
}

@article{wyart2012marginal,
  title={Marginal stability constrains force and pair distributions at random close packing},
  author={Wyart, Matthieu},
  journal={Physical review letters},
  volume={109},
  number={12},
  pages={125502},
  year={2012},
  publisher={APS}
}



@article{degiuli2015unified,
  title={Unified theory of inertial granular flows and non-Brownian suspensions},
  author={DeGiuli, E and D{\"u}ring, G and Lerner, E and Wyart, M},
  journal={Physical Review E},
  volume={91},
  number={6},
  pages={062206},
  year={2015},
  publisher={APS}
}

@article{paccolat2020isotropic,
	title = {How isotropic kernels perform on simple invariants},
	volume = {2},
	issn = {2632-2153},
	url = {https://doi.org/10.1088/2632-2153/abd485},
	doi = {10.1088/2632-2153/abd485},
	abstract = {We investigate how the training curve of isotropic kernel methods depends on the symmetry of the task to be learned, in several settings. (i) We consider a regression task, where the target function is a Gaussian random field that depends only on variables, fewer than the input dimension d. We compute the expected test error ϵ that follows where p is the size of the training set. We find that β ∼ 1/d independently of , supporting previous findings that the presence of invariants does not resolve the curse of dimensionality for kernel regression. (ii) Next we consider support-vector binary classification and introduce the stripe model, where the data label depends on a single coordinate , corresponding to parallel decision boundaries separating labels of different signs, and consider that there is no margin at these interfaces. We argue and confirm numerically that, for large bandwidth, , where ξ ∈ (0, 2) is the exponent characterizing the singularity of the kernel at the origin. This estimation improves classical bounds obtainable from Rademacher complexity. In this setting there is no curse of dimensionality since as . (iii) We confirm these findings for the spherical model, for which . (iv) In the stripe model, we show that, if the data are compressed along their invariants by some factor λ (an operation believed to take place in deep networks), the test error is reduced by a factor .},
	language = {en},
	number = {2},
	urldate = {2021-03-11},
	journal = {Machine Learning: Science and Technology},
	author = {Paccolat, Jonas and Spigler, Stefano and Wyart, Matthieu},
	month = mar,
	year = {2021},
	note = {Publisher: IOP Publishing},
	pages = {025020},
	file = {IOP Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\P56KSLF4\\Paccolat et al. - 2021 - How isotropic kernels perform on simple invariants.pdf:application/pdf},
}

@article{lerner2012unified,
  title={A unified framework for non-Brownian suspension flows and soft amorphous solids},
  author={Lerner, Edan and D{\"u}ring, Gustavo and Wyart, Matthieu},
  journal={Proceedings of the National Academy of Sciences},
  volume={109},
  number={13},
  pages={4798--4803},
  year={2012},
  publisher={National Acad Sciences}
}


@article{paccolat2020compressing,
	title = {Geometric compression of invariant manifolds in neural networks},
	volume = {2021},
	issn = {1742-5468},
	url = {https://iopscience.iop.org/article/10.1088/1742-5468/abf1f3/meta},
	doi = {10.1088/1742-5468/abf1f3},
	language = {en},
	number = {4},
	urldate = {2021-04-26},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Paccolat, Jonas and Petrini, Leonardo and Geiger, Mario and Tyloo, Kevin and Wyart, Matthieu},
	month = apr,
	year = {2021},
	note = {Publisher: IOP Publishing},
	pages = {044001},
	file = {Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\TYTMJ72W\\Paccolat et al. - 2021 - Geometric compression of invariant manifolds in ne.pdf:application/pdf;Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\NXSMV6JQ\\abf1f3.html:text/html},
}



@article{saxe2019information,
  title={On the information bottleneck theory of deep learning},
  author={Saxe, Andrew M and Bansal, Yamini and Dapello, Joel and Advani, Madhu and Kolchinsky, Artemy and Tracey, Brendan D and Cox, David D},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2019},
  number={12},
  pages={124020},
  year={2019},
  publisher={IOP Publishing}
}


@article{recanatesi2019dimensionality,
  title={Dimensionality compression and expansion in Deep Neural Networks},
  author={Recanatesi, Stefano and Farrell, Matthew and Advani, Madhu and Moore, Timothy and Lajoie, Guillaume and Shea-Brown, Eric},
  journal={arXiv preprint arXiv:1906.00443},
  year={2019}
}



@inproceedings{ansuini2019intrinsic,
  title={Intrinsic dimension of data representations in deep neural networks},
  author={Ansuini, Alessio and Laio, Alessandro and Macke, Jakob H and Zoccolan, Davide},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6111--6122},
  year={2019}
}


@article{mallat2016understanding,
  title={Understanding deep convolutional networks},
  author={Mallat, St{\'e}phane},
  journal={Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume={374},
  number={2065},
  pages={20150203},
  year={2016},
  publisher={The Royal Society Publishing}
}


@article{nguyen2020rigorous,
  title={A rigorous framework for the mean field limit of multilayer neural networks},
  author={Nguyen, Phan-Minh and Pham, Huy Tuan},
  journal={arXiv preprint arXiv:2001.11443},
  year={2020}
}

@article{zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={5th International Conference on Learning Representations, {ICLR} 2017,
              Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  year={2017}

}

@article{zhou2014object,
  title={Object detectors emerge in deep scene cnns},
  author={Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  journal={arXiv preprint arXiv:1412.6856},
  year={2014}
}
@inproceedings{kayhan2020translation,
  title={On translation invariance in cnns: Convolutional layers can exploit absolute spatial location},
  author={Kayhan, Osman Semih and Gemert, Jan C van},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14274--14285},
  year={2020}
}
@book{kardar2007statistical,
  title={Statistical physics of fields},
  author={Kardar, Mehran},
  year={2007},
  publisher={Cambridge University Press}
}
@article{bietti2019inductive,
  title={On the inductive bias of neural tangent kernels},
  author={Bietti, Alberto and Mairal, Julien},
  journal={arXiv preprint arXiv:1905.12173},
  year={2019}
}
@article{bietti2019group,
  title={Group invariance, stability to deformations, and complexity of deep convolutional representations},
  author={Bietti, Alberto and Mairal, Julien},
  journal={The Journal of Machine Learning Research},
  volume={20},
  number={1},
  pages={876--924},
  year={2019},
  publisher={JMLR. org}
}


@article{refinetti2021classifying,
  title={Classifying high-dimensional Gaussian mixtures: Where kernel methods fail and neural networks succeed},
  author={Refinetti, Maria and Goldt, Sebastian and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  journal={arXiv preprint arXiv:2102.11742},
  year={2021}
}

@inproceedings{le2013building,
  title={Building high-level features using large scale unsupervised learning},
  author={Le, Quoc V},
  booktitle={2013 IEEE international conference on acoustics, speech and signal processing},
  pages={8595--8598},
  year={2013},
  organization={IEEE}
}

@article{maennel2018gradient,
  title={Gradient descent quantizes relu network features},
  author={Maennel, Hartmut and Bousquet, Olivier and Gelly, Sylvain},
  journal={arXiv preprint arXiv:1803.08367},
  year={2018}
}


@article{fawzi2017robustness,
  title={The robustness of deep networks: A geometrical perspective},
  author={Fawzi, Alhussein and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal},
  journal={IEEE Signal Processing Magazine},
  volume={34},
  number={6},
  pages={50--62},
  year={2017},
  publisher={IEEE}
}


@inproceedings{stich2018sparsified,
  title={Sparsified SGD with memory},
  author={Stich, Sebastian U and Cordonnier, Jean-Baptiste and Jaggi, Martin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4447--4458},
  year={2018}
}

@inproceedings{gluch2020constructing,
  title={Constructing a provably adversarially-robust classifier from a high accuracy one},
  author={Gluch, Grzegorz and Urbanke, R{\"u}diger},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3674--3684},
  year={2020}
}


@article{abbe2020poly,
  title={Poly-time universality and limitations of deep learning},
  author={Abbe, Emmanuel and Sandon, Colin},
  journal={arXiv preprint arXiv:2001.02992},
  year={2020}
}

@inproceedings{goldt2019dynamics,
  title={Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup},
  author={Goldt, Sebastian and Advani, Madhu and Saxe, Andrew M and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6981--6991},
  year={2019}
}

@inproceedings{d2020double,
	title = {Double {Trouble} in {Double} {Descent}: {Bias} and {Variance}(s) in the {Lazy} {Regime}},
	shorttitle = {Double {Trouble} in {Double} {Descent}},
	url = {http://proceedings.mlr.press/v119/d-ascoli20a.html},
	abstract = {Deep neural networks can achieve remarkable generalization performances while interpolating the training data. Rather than the U-curve emblematic of the bias-variance trade-off, their test error of...},
	language = {en},
	urldate = {2020-12-29},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {D’Ascoli, Stéphane and Refinetti, Maria and Biroli, Giulio and Krzakala, Florent},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {2280--2290},
	file = {Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\SUUJKK5I\\D’Ascoli et al. - 2020 - Double Trouble in Double Descent Bias and Varianc.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\6GZLNP5X\\d-ascoli20a.html:text/html}
}

@article{dietler2020yeaz,
  title={YeaZ: A convolutional neural network for highly accurate, label-free segmentation of yeast microscopy images},
  author={Dietler, Nicola and Minder, Matthias and Gligorovski, Vojislav and Economou, Augoustina Maria and Joly, Denis Alain Henri Lucien and Sadeghi, Ahmad and Chan, Chun Hei Michael and Kozi{\'n}ski, Mateusz and Weigert, Martin and Bitbol, Anne-Florence and others},
  journal={bioRxiv},
  year={2020},
  publisher={Cold Spring Harbor Laboratory}
}

@article{vstefko2018autonomous,
  title={Autonomous illumination control for localization microscopy},
  author={{\v{S}}tefko, Marcel and Ottino, Baptiste and Douglass, Kyle M and Manley, Suliana},
  journal={Optics express},
  volume={26},
  number={23},
  pages={30882--30900},
  year={2018},
  publisher={Optical Society of America}
}

@article{barbier2019adaptive,
  title={The adaptive interpolation method: a simple scheme to prove replica formulas in Bayesian inference},
  author={Barbier, Jean and Macris, Nicolas},
  journal={Probability theory and related fields},
  volume={174},
  number={3-4},
  pages={1133--1185},
  year={2019},
  publisher={Springer}
}

@article{de2016comparing,
  title={Comparing molecules and solids across structural and alchemical space},
  author={De, Sandip and Bart{\'o}k, Albert P and Cs{\'a}nyi, G{\'a}bor and Ceriotti, Michele},
  journal={Physical Chemistry Chemical Physics},
  volume={18},
  number={20},
  pages={13754--13769},
  year={2016},
  publisher={Royal Society of Chemistry}
}


@article{van2017learning,
  title={Learning phase transitions by confusion},
  author={Van Nieuwenburg, Evert PL and Liu, Ye-Hua and Huber, Sebastian D},
  journal={Nature Physics},
  volume={13},
  number={5},
  pages={435--439},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{carleo2017solving,
  title={Solving the quantum many-body problem with artificial neural networks},
  author={Carleo, Giuseppe and Troyer, Matthias},
  journal={Science},
  volume={355},
  number={6325},
  pages={602--606},
  year={2017},
  publisher={American Association for the Advancement of Science}
}


@article{huval2015empirical,
  title={An empirical evaluation of deep learning on highway driving},
  author={Huval, Brody and Wang, Tao and Tandon, Sameep and Kiske, Jeff and Song, Will and Pazhayampallil, Joel and Andriluka, Mykhaylo and Rajpurkar, Pranav and Migimatsu, Toki and Cheng-Yue, Royce and others},
  journal={arXiv preprint arXiv:1504.01716},
  year={2015}
}


@inproceedings{amodei2016deep,
  title={Deep speech 2: End-to-end speech recognition in english and mandarin},
  author={Amodei, Dario and Ananthanarayanan, Sundaram and Anubhai, Rishita and Bai, Jingliang and Battenberg, Eric and Case, Carl and Casper, Jared and Catanzaro, Bryan and Cheng, Qiang and Chen, Guoliang and others},
  booktitle={International conference on machine learning},
  pages={173--182},
  year={2016}
}


@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@article{shi2016end,
  title={An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition},
  author={Shi, Baoguang and Bai, Xiang and Yao, Cong},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={39},
  number={11},
  pages={2298--2304},
  year={2016},
  publisher={IEEE}
}

@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{marr1979computational,
  title={A computational theory of human stereo vision},
  author={Marr, David and Poggio, Tomaso},
  journal={Proceedings of the Royal Society of London. Series B. Biological Sciences},
  volume={204},
  number={1156},
  pages={301--328},
  year={1979},
  publisher={The Royal Society London}
}


@article{dieleman2016exploiting,
  title={Exploiting cyclic symmetry in convolutional neural networks},
  author={Dieleman, Sander and De Fauw, Jeffrey and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1602.02660},
  year={2016}
}

@article{zhang2019making,
  title={Making convolutional networks shift-invariant again},
  author={Zhang, Richard},
  journal={arXiv preprint arXiv:1904.11486},
  year={2019}
}


@article{azulay2018deep,
  title={Why do deep convolutional networks generalize so poorly to small image transformations?},
  author={Azulay, Aharon and Weiss, Yair},
  journal={arXiv preprint arXiv:1805.12177},
  year={2018}
}

@online{xiao2017/online,
  author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},
  title        = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  date         = {2017-08-28},
  year         = {2017},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  eprint       = {cs.LG/1708.07747},
}

@InProceedings{pmlr-v97-simsekli19a,
  title = 	 {A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks},
  author = 	 {Simsekli, Umut and Sagun, Levent and Gurbuzbalaban, Mert},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5827--5837},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/simsekli19a/simsekli19a.pdf},
  url = 	 {http://proceedings.mlr.press/v97/simsekli19a.html},
  abstract = 	 {The gradient noise (GN) in the stochastic gradient descent (SGD) algorithm is often considered to be Gaussian in the large data regime by assuming that the classical central limit theorem (CLT) kicks in. This assumption is often made for mathematical convenience, since it enables SGD to be analyzed as a stochastic differential equation (SDE) driven by a Brownian motion. We argue that the Gaussianity assumption might fail to hold in deep learning settings and hence render the Brownian motion-based analyses inappropriate. Inspired by non-Gaussian natural phenomena, we consider the GN in a more general context and invoke the generalized CLT (GCLT), which suggests that the GN converges to a heavy-tailed $\alpha$-stable random variable. Accordingly, we propose to analyze SGD as an SDE driven by a Lévy motion. Such SDEs can incur ?jumps?, which force the SDE transition from narrow minima to wider minima, as proven by existing metastability theory. To validate the $\alpha$-stable assumption, we conduct experiments on common deep learning scenarios and show that in all settings, the GN is highly non-Gaussian and admits heavy-tails. We investigate the tail behavior in varying network architectures and sizes, loss functions, and datasets. Our results open up a different perspective and shed more light on the belief that SGD prefers wide minima.}
}

@article{lee2020finite,
  title={Finite Versus Infinite Neural Networks: an Empirical Study},
  author={Lee, Jaehoon and Schoenholz, Samuel S and Pennington, Jeffrey and Adlam, Ben and Xiao, Lechao and Novak, Roman and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:2007.15801},
  year={2020}
}

@INPROCEEDINGS{Zagoruyko2016WRN,
    author = {Sergey Zagoruyko and Nikos Komodakis},
    title = {Wide Residual Networks},
    booktitle = {BMVC},
    year = {2016}}

@inproceedings{
yaida2018fluctuationdissipation,
title={Fluctuation-dissipation relations for stochastic gradient descent},
author={Sho Yaida},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=SkNksoRctQ},
}

@inproceedings{chizat2019lazy,
  title={On lazy training in differentiable programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2937--2947},
  year={2019}
}
@incollection{Chizat2018,
title = {{On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport}},
author = {Chizat, L\'{e}na\"{\i}c and Bach, Francis},
booktitle = {Advances in Neural Information Processing Systems 31},
pages = {3040--3050},
year = {2018},
publisher = {Curran Associates, Inc.},
}

@article{nguyen2019mean,
  title={Mean Field Limit of the Learning Dynamics of Multilayer Neural Networks},
  author={Nguyen, Phan-Minh},
  journal={arXiv preprint arXiv:1902.02880},
  year={2019}
}
@article{Dyer19,
  title={Asymptotics of wide networks from feynman diagrams},
  author={Dyer, Ethan and Gur-Ari, Guy},
  journal={arXiv preprint arXiv:1909.11304},
  year={2019}
}

@inproceedings{
Du2019,
title={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
author={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=S1eK3i09YQ},
}

@inproceedings{Allen-Zhu2018,
	title = {A {Convergence} {Theory} for {Deep} {Learning} via {Over}-{Parameterization}},
	url = {http://proceedings.mlr.press/v97/allen-zhu19a.html},
	abstract = {Deep neural networks (DNNs) have demonstrated dominating performance in many fields; since AlexNet, networks used in practice are going wider and deeper. On the theoretical side, a long line of wor...},
	language = {en},
	urldate = {2020-09-29},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {242--252},
	file = {Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\AWHFMRLR\\Allen-Zhu et al. - 2019 - A Convergence Theory for Deep Learning via Over-Pa.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\CJIZ8ZNU\\allen-zhu19a.html:text/html}
}


@article{lecun-mnist,
  added-at = {2010-06-28T21:16:30.000+0200},
  author = {LeCun, Yann and Cortes, Corinna},
  biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {MSc _checked character_recognition mnist network neural},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {{MNIST} handwritten digit database},
  url = {http://yann.lecun.com/exdb/mnist/},
  username = {mhwombat},
  year = 2010,
  journal = {},
}




@book{scholkopf2001learning,
  title={Learning with kernels: support vector machines, regularization, optimization, and beyond},
  author={Scholkopf, Bernhard and Smola, Alexander J},
  year={2001},
  publisher={MIT press}
}

@book{gikhman2015theory,
  title={The theory of stochastic processes I},
  author={Gikhman, Iosif I and Skorokhod, Anatoli V},
  year={2015},
  publisher={Springer}
}

@article{saad1995line,
  title={On-line learning in soft committee machines},
  author={Saad, David and Solla, Sara A},
  journal={Physical Review E},
  volume={52},
  number={4},
  pages={4225},
  year={1995},
  publisher={APS}
}

@book{opper2001advanced,
  title={Advanced mean field methods: Theory and practice},
  author={Opper, Manfred and Saad, David},
  year={2001},
  publisher={MIT press}
}

@book{williams2006gaussian,
  title={Gaussian processes for machine learning},
  author={Williams, Christopher KI and Rasmussen, Carl Edward},
  volume={2},
  number={3},
  year={2006},
  publisher={MIT Press Cambridge, MA}
}

@article{smola1998connection,
  title={The connection between regularization operators and support vector kernels},
  author={Smola, Alex J and Sch{\"o}lkopf, Bernhard and M{\"u}ller, Klaus-Robert},
  journal={Neural networks},
  volume={11},
  number={4},
  pages={637--649},
  year={1998},
  publisher={Elsevier}
}

@article{stein1999predicting,
  title={Predicting random fields with increasing dense observations},
  author={Stein, Michael L and others},
  journal={The Annals of Applied Probability},
  volume={9},
  number={1},
  pages={242--273},
  year={1999},
  publisher={Institute of Mathematical Statistics}
}
@article{de2020sparsity,
  title={On Sparsity in Overparametrised Shallow ReLU Networks},
  author={de Dios, Jaume and Bruna, Joan},
  journal={arXiv preprint arXiv:2006.10225},
  year={2020}
}

@article{bruna2013invariant,
  title={Invariant scattering convolution networks},
  author={Bruna, Joan and Mallat, St{\'e}phane},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={8},
  pages={1872--1886},
  year={2013},
  publisher={IEEE}
}
@article{lowe2004distinctive,
  title={Distinctive image features from scale-invariant keypoints},
  author={Lowe, David G},
  journal={International journal of computer vision},
  volume={60},
  number={2},
  pages={91--110},
  year={2004},
  publisher={Springer}
}
@book{stein2012interpolation,
  title={Interpolation of spatial data: some theory for kriging},
  author={Stein, Michael L},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{hestness2017deep,
  title={Deep learning scaling is predictable, empirically},
  author={Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md and Ali, Mostofa and Yang, Yang and Zhou, Yanqi},
  journal={arXiv preprint arXiv:1712.00409},
  year={2017}
}

@article{luxburg2004distance,
  title={Distance-based classification with Lipschitz functions},
  author={Luxburg, Ulrike von and Bousquet, Olivier},
  journal={Journal of Machine Learning Research},
  volume={5},
  number={Jun},
  pages={669--695},
  year={2004}
}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

@inproceedings{rudi2017generalization,
  title={Generalization properties of learning with random features},
  author={Rudi, Alessandro and Rosasco, Lorenzo},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3215--3225},
  year={2017}
}

@inproceedings{mei2019mean,
	title = {Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit},
	shorttitle = {Mean-field theory of two-layers neural networks},
	url = {http://proceedings.mlr.press/v99/mei19a.html},
	abstract = {We consider learning two layer neural networks using stochastic gradient descent. The mean-field description of this learning dynamics approximates the evolution of the network weights by an evolut...},
	language = {en},
	urldate = {2020-12-30},
	booktitle = {Conference on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
	month = jun,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {2388--2464},
	file = {Snapshot:C\:\\Users\\leope\\Zotero\\storage\\VNYZQZI9\\mei19a.html:text/html;Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\UGUDWNS9\\Mei et al. - 2019 - Mean-field theory of two-layers neural networks d.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\U62KYIE2\\mei19a.html:text/html}
}



@article{ikeda2005asymptotic,
  title={An asymptotic statistical analysis of support vector machines with soft margins},
  author={Ikeda, Kazushi and Aoishi, Tsutomu},
  journal={Neural Networks},
  volume={18},
  number={3},
  pages={251--259},
  year={2005},
  publisher={Elsevier}
}

@article{amari1993universal,
  title={A universal theorem on learning curves},
  author={Amari, Shun-Ichi},
  journal={Neural networks},
  volume={6},
  number={2},
  pages={161--166},
  year={1993},
  publisher={Elsevier}
}

@article{amari1992four,
  title={Four types of learning curves},
  author={Amari, Shun-ichi and Fujita, Naotake and Shinomoto, Shigeru},
  journal={Neural Computation},
  volume={4},
  number={4},
  pages={605--618},
  year={1992},
  publisher={MIT Press}
}



@article{Geiger18,
	title = {Jamming transition as a paradigm to understand the loss landscape of deep neural networks},
	volume = {100},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.100.012115},
	doi = {10.1103/PhysRevE.100.012115},
	abstract = {Deep learning has been immensely successful at a variety of tasks, ranging from classification to artificial intelligence. Learning corresponds to fitting training data, which is implemented by descending a very high-dimensional loss function. Understanding under which conditions neural networks do not get stuck in poor minima of the loss, and how the landscape of that loss evolves as depth is increased, remains a challenge. Here we predict, and test empirically, an analogy between this landscape and the energy landscape of repulsive ellipses. We argue that in fully connected deep networks a phase transition delimits the over- and underparametrized regimes where fitting can or cannot be achieved. In the vicinity of this transition, properties of the curvature of the minima of the loss (the spectrum of the Hessian) are critical. This transition shares direct similarities with the jamming transition by which particles form a disordered solid as the density is increased, which also occurs in certain classes of computational optimization and learning problems such as the perceptron. Our analysis gives a simple explanation as to why poor minima of the loss cannot be encountered in the overparametrized regime. Interestingly, we observe that the ability of fully connected networks to fit random data is independent of their depth, an independence that appears to also hold for real data. We also study a quantity Δ which characterizes how well (Δ{\textless}0) or badly (Δ{\textgreater}0) a datum is learned. At the critical point it is power-law distributed on several decades, P+(Δ)∼Δθ for Δ{\textgreater}0 and P−(Δ)∼(−Δ)−γ for Δ{\textless}0, with exponents that depend on the choice of activation function. This observation suggests that near the transition the loss landscape has a hierarchical structure and that the learning dynamics is prone to avalanche-like dynamics, with abrupt changes in the set of patterns that are learned.},
	number = {1},
	urldate = {2020-09-29},
	journal = {Physical Review E},
	author = {Geiger, Mario and Spigler, Stefano and d'Ascoli, Stéphane and Sagun, Levent and Baity-Jesi, Marco and Biroli, Giulio and Wyart, Matthieu},
	month = jul,
	year = {2019},
	note = {Publisher: American Physical Society},
	pages = {012115},
	file = {Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\NX8283BB\\Geiger et al. - 2019 - Jamming transition as a paradigm to understand the.pdf:application/pdf;APS Snapshot:C\:\\Users\\leope\\Zotero\\storage\\VWT9YRFC\\PhysRevE.100.html:text/html}
}

@inproceedings{domingos00,
  title={A unified bias-variance decomposition},
  author={Domingos, Pedro},
  booktitle={Proceedings of 17th International Conference on Machine Learning},
  pages={231--238},
  year={2000}
}


@string{epje = {Eur.\ Phys.\ J E}}

@string{epl = {Europhys.\ Lett.}}

@string{pnas = {Proc.\ Natl.\ Acad.\ Sci.\ U.S.A.}}

@string{prb = {Phys.\ Rev.\ B}}

@string{pre = {Phys.\ Rev.\ E}}

@string{prl = {Phys.\ Rev.\ Lett.}}

@string{rmp = {Rev.\ Mod.\ Phys.}}

@inproceedings{jacot2018neural,
 author = {Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
 title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
 booktitle = {Proceedings of the 32Nd International Conference on Neural Information Processing Systems},
 series = {NIPS'18},
 year = {2018},
 location = {Montr\&\#233;al, Canada},
 pages = {8580--8589},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=3327757.3327948},
 acmid = {3327948},
 publisher = {Curran Associates Inc.},
 address = {USA},
} 

@article{jacot2019hessian,
  title={The asymptotic spectrum of the Hessian of DNN throughout training},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={arXiv preprint arXiv:1910.02875},
  year={2019}
}

@article{jacot2019hessian2,
  title={The Neural Tangent Kernel describes the Hessian of Overparametrized DNNs},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
  year={2019},
  journal={},
}

@inproceedings{williams1997computing,
  title={Computing with infinite networks},
  author={Williams, Christopher KI},
  booktitle={Advances in neural information processing systems},
  pages={295--301},
  year={1997}
}

@inproceedings{
novak2018bayesian,
title={Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes},
author={Roman Novak and Lechao Xiao and Yasaman Bahri and Jaehoon Lee and Greg Yang and Daniel A. Abolafia and Jeffrey Pennington and Jascha Sohl-dickstein},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=B1g30j0qF7},
}

@article{Lee2017,
  title={Deep Neural Networks as Gaussian Processes},
  author={Jae Hoon Lee and Yasaman Bahri and Roman Novak and Samuel S. Schoenholz and Jeffrey Pennington and Jascha Sohl-Dickstein},
  journal={ICLR},
  year={2018}
}

@book{Neal1996,
 author = {Neal, Radford M.},
 title = {Bayesian Learning for Neural Networks},
 year = {1996},
 isbn = {0387947248},
 publisher = {Springer-Verlag New York, Inc.},
 address = {Secaucus, NJ, USA},
} 

@incollection{Cho2009,
title = {Kernel Methods for Deep Learning},
author = {Youngmin Cho and Lawrence K. Saul},
booktitle = {Advances in Neural Information Processing Systems 22},
pages = {342--350},
year = {2009},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf}
}

@inproceedings{matthews2018gaussian,
  title={Gaussian Process Behaviour in Wide Deep Neural Networks},
  author={Alexander G. de G. Matthews and Jiri Hron and Mark Rowland and Richard E. Turner and Zoubin Ghahramani},
  booktitle={International Conference on Learning Representations},
  year={2018},
  url={https://openreview.net/forum?id=H1-nGgWC-},
}

@article{neal2018modern,
  title={A modern take on the bias-variance tradeoff in neural networks},
  author={Neal, Brady and Mittal, Sarthak and Baratin, Aristide and Tantia, Vinayak and Scicluna, Matthew and Lacoste-Julien, Simon and Mitliagkas, Ioannis},
  journal={arXiv preprint arXiv:1810.08591},
  year={2018}
}

@article{Spigler18, 
	title = {A jamming transition from under- to over-parametrization affects generalization in deep learning},
	volume = {52},
	issn = {1751-8121},
	url = {https://iopscience.iop.org/article/10.1088/1751-8121/ab4c8b/meta},
	doi = {10.1088/1751-8121/ab4c8b},
	language = {en},
	number = {47},
	urldate = {2020-09-29},
	journal = {Journal of Physics A: Mathematical and Theoretical},
	author = {Spigler, S. and Geiger, M. and d’Ascoli, S. and Sagun, L. and Biroli, G. and Wyart, M.},
	month = oct,
	year = {2019},
	note = {Publisher: IOP Publishing},
	pages = {474001},
	file = {Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\7FXTMGCL\\Spigler et al. - 2019 - A jamming transition from under- to over-parametri.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\TTZP2QI8\\meta.html:text/html}
}


@article{Zdeborova07,
  title={Phase transitions in the coloring of random graphs},
  author={{Zdeborov{\'a}}, Lenka and Krzakala, Florent},
  journal={Physical Review E},
  volume={76},
  number={3},
  pages={031131},
  year={2007},
  publisher={APS}
}

@article{Krzakala07,
  title={Landscape analysis of constraint satisfaction problems},
  author={Krzakala, Florent and Kurchan, Jorge},
  journal={Physical Review E},
  volume={76},
  number={2},
  pages={021122},
  year={2007},
  publisher={APS}
}

@article{Monasson95,
  title={Weight space structure and internal representations: a direct approach to learning and generalization in multilayer neural networks},
  author={Monasson, {R{\'e}mi} and Zecchina, Riccardo},
  journal={Physical review letters},
  volume={75},
  number={12},
  pages={2432},
  year={1995},
  publisher={APS}
}

@article{Cooper18,
  title={The loss landscape of overparameterized neural networks},
  author={Cooper, Yaim},
  journal={arXiv preprint arXiv:1804.10200},
  year={2018}
}

@inproceedings{eldan2016power,
  title={The power of depth for feedforward neural networks},
  author={Eldan, Ronen and Shamir, Ohad},
  booktitle={Conference on Learning Theory},
  pages={907--940},
  year={2016}
}



@article{Li18,
  title={Measuring the intrinsic dimension of objective landscapes},
  author={Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and Yosinski, Jason},
  journal={arXiv preprint arXiv:1804.08838},
  year={2018}
}

@article{Lerner12,
	Author = {E. Lerner and G. {D\"uring} and M. Wyart},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-09-05 15:59:57 +0000},
	Journal = {EPL (Europhysics Letters)},
	Number = {5},
	Pages = {58003},
	Title = {Toward a microscopic description of flow near the jamming threshold},
	Volume = {99},
	Year = {2012},
	Bdsk-Url-1 = {http://stacks.iop.org/0295-5075/99/i=5/a=58003}}
	
@article{Charbonneau12,
	Author = {Charbonneau, Patrick and Corwin, Eric I. and Parisi, Giorgio and Zamponi, Francesco},
	Date = {2012/11/13/},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-05-22 20:13:33 +0000},
	Day = {13},
	Id = {10.1103/PhysRevLett.109.205501},
	J1 = {PRL},
	Journal = {Physical Review Letters},
	Journal1 = {Phys. Rev. Lett.},
	Month = {11},
	Number = {20},
	Pages = {205501--},
	Publisher = {American Physical Society},
	Title = {Universal Microstructure and Mechanical Stability of Jammed Packings},
	Ty = {JOUR},
	Url = {http://link.aps.org/doi/10.1103/PhysRevLett.109.205501},
	Volume = {109},
	Year = {2012},
	Bdsk-Url-1 = {http://link.aps.org/doi/10.1103/PhysRevLett.109.205501}
}

@article{yang2019scaling,
  title={Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation},
  author={Yang, Greg},
  journal={arXiv preprint arXiv:1902.04760},
  year={2019}
}

@incollection{lee2019wide,
	title = {Wide {Neural} {Networks} of {Any} {Depth} {Evolve} as {Linear} {Models} {Under} {Gradient} {Descent}},
	url = {http://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.pdf},
	urldate = {2020-09-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {8572--8583},
	file = {NIPS Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\9XQ8IXMY\\Lee et al. - 2019 - Wide Neural Networks of Any Depth Evolve as Linear.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\leope\\Zotero\\storage\\DHGXIH9D\\9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent.html:text/html}
}

@inproceedings{daniely2016toward,
  title={Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity},
  author={Daniely, Amit and Frostig, Roy and Singer, Yoram},
  booktitle={Advances In Neural Information Processing Systems},
  pages={2253--2261},
  year={2016}
}
@article{rotskoff2018neural,
  title={Neural networks as Interacting Particle Systems: Asymptotic convexity of the Loss Landscape and Universal Scaling of the Approximation Error},
  author={Rotskoff, Grant M and Vanden-Eijnden, Eric},
  journal={arXiv preprint arXiv:1805.00915},
  year={2018}
}

@article{ghorbani2019linearized,
  title={Linearized two-layers neural networks in high dimension},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={arXiv preprint arXiv:1904.12191},
  year={2019}
}

@article{ongie2019function,
	title = {A {Function} {Space} {View} of {Bounded} {Norm} {Infinite} {Width} {ReLU} {Nets}: {The} {Multivariate} {Case}},
	shorttitle = {A {Function} {Space} {View} of {Bounded} {Norm} {Infinite} {Width} {ReLU} {Nets}},
	url = {https://iclr.cc/virtual_2020/poster_H1lNPxHKDH.html},
	abstract = {We give a tight characterization of the (vectorized Euclidean) norm of weights required to realize a function \$f:{\textbackslash}mathbb\{R\}{\textbackslash}rightarrow {\textbackslash}mathbb\{R\}{\textasciicircum}d\$ as a single hidden-layer ReLU network with an unbounded number of units (infinite width), extending the univariate characterization of Savarese et al. (2019) to the multivariate case.},
	journal = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
	language = {en},
	urldate = {2020-12-30},
	author = {Ongie, Greg and Willett, Rebecca and Soudry, Daniel and Srebro, Nathan},
	month = apr,
	year = {2020},
	file = {Snapshot:C\:\\Users\\leope\\Zotero\\storage\\LCTGXRBW\\poster_H1lNPxHKDH.html:text/html;Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\US3XSNVQ\\Ongie et al. - 2020 - A Function Space View of Bounded Norm Infinite Wid.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\TV9PTDNU\\poster_H1lNPxHKDH.html:text/html}
}

@article{Lerner13a,
	Abstract = {We study theoretically and numerically how hard frictionless particles in random packings can rearrange. We demonstrate the existence of two distinct unstable non-linear modes of rearrangement{,} both associated with the opening and the closing of contacts. The first mode{,} whose density is characterized by some exponent [small theta][prime or minute]{,} corresponds to motions of particles extending throughout the entire system. The second mode{,} whose density is characterized by an exponent [small theta] [not equal] [small theta][prime or minute]{,} corresponds to the local buckling of a few particles. Extended modes are shown to yield at a much higher rate than local modes when a stress is applied. We show that the distribution of contact forces follows P(f) [similar] fmin([small theta][prime or minute]{,}[small theta]){,} and that imposing the restriction that the packing cannot be densified further leads to the bounds and {,} where [gamma] characterizes the singularity of the pair distribution function g(r) at contact. These results extend the theoretical analysis of [Wyart{,} Phys. Rev. Lett.{,} 2012{,} 109{,} 125502] where the existence of local modes was not considered. We perform numerics that support that these bounds are saturated with [gamma] [approximate] 0.38{,} [small theta] [approximate] 0.17 and [small theta][prime or minute] [approximate] 0.44. We measure systematically the stability of all such modes in packings{,} and confirm their marginal stability. The principle of marginal stability thus allows us to make clearcut predictions on the ensemble of configurations visited in these out-of-equilibrium systems{,} and on the contact forces and pair distribution functions. It also reveals the excitations that need to be included in a description of plasticity or flow near jamming{,} and suggests a new path to study two-level systems and soft spots in simple amorphous solids of repulsive particles.},
	Author = {Lerner, Edan and During, Gustavo and Wyart, Matthieu},
	Date-Added = {2014-05-22 20:15:23 +0000},
	Date-Modified = {2014-09-05 16:00:04 +0000},
	Doi = {10.1039/C3SM50515D},
	Issue = {34},
	Journal = {Soft Matter},
	Pages = {8252-8263},
	Publisher = {The Royal Society of Chemistry},
	Title = {Low-energy non-linear excitations in sphere packings},
	Volume = {9},
	Year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1039/C3SM50515D}}

@article{Saxe13,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={International Conference on Learning Representations},
  year={2014}
}

@article{Kingma14,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={International Conference on Learning Representations},
  year={2015}
}

@article{Ohern03,
	Author = {O'Hern, Corey S. and Silbert, Leonardo E. and Liu, Andrea J. and Nagel, Sidney R.},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-11-09 08:34:09 +0000},
	Doi = {10.1103/PhysRevE.68.011306},
	Journal = {Phys. Rev. E},
	Month = {Jul},
	Number = {1},
	Pages = {011306--011324},
	Publisher = {American Physical Society},
	Title = {Jamming at zero temperature and zero applied stress: The epitome of disorder},
	Volume = {68},
	Year = {2003},
	Bdsk-Url-1 = {http://link.aps.org/doi/10.1103/PhysRevE.68.011306},
	Bdsk-Url-2 = {http://dx.doi.org/10.1103/PhysRevE.68.011306}}

@article{Balduzzi17,
  title={The Shattered Gradients Problem: If resnets are the answer, then what is the question?},
  author={Balduzzi, David and Frean, Marcus and Leary, Lennox and Lewis, JP and Ma, Kurt Wan-Duo and McWilliams, Brian},
  journal={arXiv preprint arXiv:1702.08591},
  year={2017}
}

@article{Balduzzi16,
  title={Deep online convex optimization with gated games},
  author={Balduzzi, David},
  journal={arXiv preprint arXiv:1604.01952},
  year={2016}
}

@book{Phillips81,
	Author = {Anderson, A.C.},
	Date-Added = {2014-06-13 22:47:59 +0000},
	Date-Modified = {2015-06-04 02:59:24 +0000},
	Editor = {W. A. Phillips},
	Publisher = {Springer, Berlin},
	Series = {Topics in Current Physics},
	Title = {Amorphous Solids: Low Temperature Properties},
	Volume = {24},
	Year = {1981}
}

@article{During13,
	Author = {{D{\"u}ring}, Gustavo and Lerner, Edan and Wyart, Matthieu},
	Date = {2013},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-05-22 20:13:33 +0000},
	Journal = {Soft Matter},
	Number = {1},
	Pages = {146-154},
	Publisher = {Royal Society of Chemistry},
	Title = {Phonon gap and localization lengths in floppy materials},
	Volume = {9},
	Year = {2013}}

@article{DeGiuli14,
	Author = {DeGiuli, Eric and Laversanne-Finot, Adrien and {D\"uring}, Gustavo Alberto and Lerner, Edan and Wyart, Matthieu},
	Date = {2014},
	Date-Added = {2014-07-08 08:27:13 +0000},
	Date-Modified = {2014-07-10 08:54:49 +0000},
	Journal = {Soft Matter},
	Number = {30},
	Pages = {5628-5644},
	Publisher = {Royal Society of Chemistry},
	Title = {Effects of coordination and pressure on sound attenuation, boson peak and elasticity in amorphous solids},
	Volume = {10},
	Year = {2014}}

@article{Yan16,
	Author = {Yan, Le and DeGiuli, Eric and Wyart, Matthieu},
	Date = {2016},
	Date-Added = {2016-10-13 12:49:09 +0000},
	Date-Modified = {2016-10-13 12:49:27 +0000},
	Journal = {EPL (Europhysics Letters)},
	Number = {2},
	Pages = {26003},
	Publisher = {IOP Publishing},
	Title = {On variational arguments for vibrational modes near jamming},
	Volume = {114},
	Year = {2016}}


@article{Tkachenko99,
	Author = {Tkachenko, Alexei V. and Witten, Thomas A.},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-05-22 20:13:34 +0000},
	Doi = {10.1103/PhysRevE.60.687},
	Journal = {Phys. Rev. E},
	Month = {Jul},
	Number = {1},
	Numpages = {9},
	Pages = {687--696},
	Publisher = {American Physical Society},
	Title = {Stress propagation through frictionless granular material},
	Volume = {60},
	Year = {1999},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevE.60.687}}

@article{Baum88,
  title={On the capabilities of multilayer perceptrons},
  author={Baum, Eric B},
  journal={Journal of complexity},
  volume={4},
  number={3},
  pages={193--215},
  year={1988},
  publisher={Academic Press}
}

@article{Gardner88,
  title={The space of interactions in neural network models},
  author={Gardner, Elizabeth},
  journal={Journal of physics A: Mathematical and general},
  volume={21},
  number={1},
  pages={257},
  year={1988},
  publisher={IOP Publishing}
}

@article{Franz15,
  title={Universal spectrum of normal modes in low-temperature glasses},
  author={Franz, Silvio and Parisi, Giorgio and Urbani, Pierfrancesco and Zamponi, Francesco},
  journal={Proceedings of the National Academy of Sciences},
  volume={112},
  number={47},
  pages={14539--14544},
  year={2015},
  publisher={National Acad Sciences}
}

@article{Franz17,
  title={Universality of the SAT-UNSAT (jamming) threshold in non-convex continuous constraint satisfaction problems},
  author={Franz, Silvio and Parisi, Giorgio and Sevelev, Maxime and Urbani, Pierfrancesco and Zamponi, Francesco},
  journal={SciPost Physics},
  volume={2},
  number={3},
  pages={019},
  year={2017}
}

@article{Brito18a,
	title = {Universality of jamming of nonspherical particles},
	volume = {115},
	copyright = {© 2018 . http://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/115/46/11736},
	doi = {10.1073/pnas.1812457115},
	abstract = {Amorphous packings of nonspherical particles such as ellipsoids and spherocylinders are known to be hypostatic: The number of mechanical contacts between particles is smaller than the number of degrees of freedom, thus violating Maxwell’s mechanical stability criterion. In this work, we propose a general theory of hypostatic amorphous packings and the associated jamming transition. First, we show that many systems fall into a same universality class. As an example, we explicitly map ellipsoids into a system of “breathing” particles. We show by using a marginal stability argument that in both cases jammed packings are hypostatic and that the critical exponents related to the contact number and the vibrational density of states are the same. Furthermore, we introduce a generalized perceptron model which can be solved analytically by the replica method. The analytical solution predicts critical exponents in the same hypostatic jamming universality class. Our analysis further reveals that the force and gap distributions of hypostatic jamming do not show power-law behavior, in marked contrast to the isostatic jamming of spherical particles. Finally, we confirm our theoretical predictions by numerical simulations.},
	language = {en},
	number = {46},
	urldate = {2020-09-29},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Brito, Carolina and Ikeda, Harukuni and Urbani, Pierfrancesco and Wyart, Matthieu and Zamponi, Francesco},
	month = nov,
	year = {2018},
	pmid = {30381457},
	note = {Publisher: National Academy of Sciences
Section: Physical Sciences},
	keywords = {glass, jamming, marginal stability, nonspherical particles},
	pages = {11736--11741},
	file = {Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\URYRABRI\\Brito et al. - 2018 - Universality of jamming of nonspherical particles.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\PHCPDT94\\11736.html:text/html}
}

@article{Muller14,
	Author = {M{\"u}ller, Markus and Wyart, Matthieu},
	Date-Added = {2015-01-05 15:07:48 +0000},
	Date-Modified = {2015-06-04 20:14:00 +0000},
	Doi = {10.1146/annurev-conmatphys-031214-014614},
	Journal = {Annual Review of Condensed Matter Physics},
	Number = {1},
	Pages = {177--200},
	Title = {Marginal Stability in Structural, Spin, and Electron Glasses},
	Volume = {6},
	Year = {2015},
	Bdsk-Url-1 = {http://www.annualreviews.org/doi/abs/10.1146/annurev-conmatphys-031214-014614},
	Bdsk-Url-2 = {http://dx.doi.org/10.1146/annurev-conmatphys-031214-014614}}

@article{Charbonneau15,
	Author = {Charbonneau, Patrick and Corwin, Eric I and Parisi, Giorgio and Zamponi, Francesco},
	Date = {2015},
	Date-Added = {2015-04-16 02:52:40 +0000},
	Date-Modified = {2015-04-16 02:52:40 +0000},
	Journal = {Physical Review Letters},
	Number = {12},
	Pages = {125504},
	Publisher = {APS},
	Title = {Jamming Criticality Revealed by Removing Localized Buckling Excitations},
	Volume = {114},
	Year = {2015}}

@article{Charbonneau14,
	Author = {Charbonneau, Patrick and Kurchan, Jorge and Parisi, Giorgio and Urbani, Pierfrancesco and Zamponi, Francesco},
	Date = {2014},
	Date-Added = {2014-05-09 02:04:58 +0000},
	Date-Modified = {2014-06-13 16:56:16 +0000},
	Journal = {Nature Communications},
	Number = {3725},
	Publisher = {Nature Publishing Group},
	Title = {Fractal free energy landscapes in structural glasses},
	Volume = {5},
	Year = {2014}}

@article{Charbonneau14a,
	Author = {Charbonneau, Patrick and Kurchan, Jorge and Parisi, Giorgio and Urbani, Pierfrancesco and Zamponi, Francesco},
	Date = {2014},
	Date-Added = {2014-10-20 02:26:59 +0000},
	Date-Modified = {2014-10-20 02:27:25 +0000},
	Journal = {Journal of Statistical Mechanics: Theory and Experiment},
	Number = {10},
	Pages = {10009},
	Publisher = {IOP Publishing},
	Title = {Exact theory of dense amorphous hard spheres in high dimension. III. The full replica symmetry breaking solution},
	Volume = {2014},
	Year = {2014}}
	
@article{Wyart12,
	Author = {Wyart, Matthieu},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-09-05 15:59:36 +0000},
	Doi = {10.1103/PhysRevLett.109.125502},
	Issue = {12},
	Journal = {Phys. Rev. Lett.},
	Month = {Sep},
	Numpages = {5},
	Pages = {125502},
	Publisher = {American Physical Society},
	Title = {Marginal Stability Constrains Force and Pair Distributions at Random Close Packing},
	Volume = {109},
	Year = {2012},
	Bdsk-Url-1 = {http://link.aps.org/doi/10.1103/PhysRevLett.109.125502},
	Bdsk-Url-2 = {http://dx.doi.org/10.1103/PhysRevLett.109.125502}}

@article{Zeravcic09,
	Abstract = {We study the vibrational modes of three-dimensional jammed packings of soft ellipsoids of revolution as a function of particle aspect ratio {{\OE}µ} and packing fraction. At the jamming transition for ellipsoids, as distinct from the idealized case using spheres where {{\OE}µ=1}, there are many unconstrained and nontrivial rotational degrees of freedom. These constitute a set of zero-frequency modes that are gradually mobilized into a new rotational band as {|{\OE}µ-1|} increases. Quite surprisingly, as this new band is separated from zero frequency by a gap, and lies below the onset frequency for translational vibrations, {{\oe}{\^a} * }, the presence of these new degrees of freedom leaves unaltered the basic scenario that the translational spectrum is determined only by the average contact number. Indeed, {{\oe}{\^a} *} depends solely on coordination as it does for compressed packings of spheres. We also discuss the regime of large {|{\OE}µ-1|}, where the two bands merge.},
	Author = {Z. Zeravcic and N. Xu and A. J. Liu and S. R. Nagel and W. van Saarloos},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-05-22 20:13:34 +0000},
	Journal = EPL,
	Number = {2},
	Pages = {26001},
	Title = {Excitations of ellipsoid packings near jamming},
	Volume = {87},
	Year = {2009}}


@article{Donev04a,
	Abstract = {Packing problems, such as how densely objects can fill a volume, are among the most ancient and persistent problems in mathematics and science. For equal spheres, it has only recently been proved that the face-centered cubic lattice has the highest possible packing fraction . It is also well known that certain random (amorphous) jammed packings have {{\oe}{\"U}} ?{{\^a}{\`a}} 0.64. Here, we show experimentally and with a new simulation algorithm that ellipsoids can randomly pack more densely?{{\"A}{\^\i}}up to {{\oe}{\"U}}= 0.68 to 0.71for spheroids with an aspect ratio close to that of M&M's Candies?{{\"A}{\^\i}}and even approach {{\oe}{\"U}} ?{{\^a}{\`a}} 0.74 for ellipsoids with other aspect ratios. We suggest that the higher density is directly related to the higher number of degrees of freedom per particle and thus the larger number of particle contacts required to mechanically stabilize the packing. We measured the number of contacts per particle Z ?{{\^a}{\`a}} 10 for our spheroids, as compared to Z ?{{\^a}{\`a}} 6 for spheres. Our results have implications for a broad range of scientific disciplines, including the properties of granular media and ceramics, glass formation, and discrete geometry.},
	Author = {Donev, Aleksandar and Cisse, Ibrahim and Sachs, David and Variano, Evan A. and Stillinger, Frank H. and Connelly, Robert and Torquato, Salvatore and Chaikin, P. M.},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-05-22 20:13:33 +0000},
	Doi = {10.1126/science.1093010},
	Journal = {Science},
	Number = {5660},
	Pages = {990-993},
	Title = {Improving the Density of Jammed Disordered Packings Using Ellipsoids},
	Volume = {303},
	Year = {2004},
	Bdsk-Url-1 = {http://dx.doi.org/10.1126/science.1093010}}

@article{Franz16,
  title={The simplest model of jamming},
  author={Franz, Silvio and Parisi, Giorgio},
  journal={Journal of Physics A: Mathematical and Theoretical},
  volume={49},
  number={14},
  pages={145001},
  year={2016},
  publisher={IOP Publishing}
}



@article{Mailman09,
	Author = {Mailman, Mitch and Schreck, Carl F. and O'Hern, Corey S. and Chakraborty, Bulbul},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-05-22 20:13:33 +0000},
	Doi = {10.1103/PhysRevLett.102.255501},
	Journal = {Phys. Rev. Lett.},
	Month = {Jun},
	Number = {25},
	Numpages = {4},
	Pages = {255501},
	Publisher = {American Physical Society},
	Title = {Jamming in Systems Composed of Frictionless Ellipse-Shaped Particles},
	Volume = {102},
	Year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevLett.102.255501}}

@article{Silbert05,
	Author = {L. E. Silbert and A. J. Liu and S. R. Nagel},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-05-22 20:13:34 +0000},
	Journal = PRL,
	Pages = {098301},
	Title = {Vibrations and Diverging Length Scales Near the Unjamming Transition},
	Volume = {95},
	Year = {2005}}
	
@article{Wyart05a,
	Author = {Wyart, Matthieu and Silbert, Leonardo E and Nagel, Sidney R and Witten, Thomas A},
	Date = {2005},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-05-22 20:13:34 +0000},
	Journal = {Physical Review E},
	Number = {5},
	Pages = {051306},
	Publisher = {APS},
	Title = {Effects of compression on the vibrational modes of marginally jammed solids},
	Volume = {72},
	Year = {2005}}

@article{Lipton16,
  title={Stuck in a what? adventures in weight space},
  author={Lipton, Zachary C},
  journal={International Conference on Learning Representations},
  year={2016}
}

@article{franz2018jamming,
	title = {Jamming in {Multilayer} {Supervised} {Learning} {Models}},
	volume = {123},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.123.160602},
	doi = {10.1103/PhysRevLett.123.160602},
	abstract = {Critical jamming transitions are characterized by an astonishing degree of universality. Analytic and numerical evidence points to the existence of a large universality class that encompasses finite and infinite dimensional spheres and continuous constraint satisfaction problems (CCSP) such as the nonconvex perceptron and related models. In this Letter we investigate multilayer neural networks (MLNN) learning random associations as models for CCSP that could potentially define different jamming universality classes. As opposed to simple perceptrons and infinite dimensional spheres, which are described by a single effective field in terms of which the constraints appear to be one dimensional, the description of MLNN involves multiple fields, and the constraints acquire a multidimensional character. We first study the models numerically and show that similarly to the perceptron, whenever jamming is isostatic, the sphere universality class is recovered, we then write the exact mean-field equations for the models and identify a dimensional reduction mechanism that leads to a scaling regime identical to the one of spheres.},
	number = {16},
	urldate = {2020-09-29},
	journal = {Physical Review Letters},
	author = {Franz, Silvio and Hwang, Sungmin and Urbani, Pierfrancesco},
	month = oct,
	year = {2019},
	note = {Publisher: American Physical Society},
	pages = {160602},
	file = {Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\BLJF3H85\\Franz et al. - 2019 - Jamming in Multilayer Supervised Learning Models.pdf:application/pdf;APS Snapshot:C\:\\Users\\leope\\Zotero\\storage\\EH69D4MZ\\PhysRevLett.123.html:text/html}
}

@article{Franz17b,
  title={Mean-field avalanches in jammed spheres},
  author={Franz, Silvio and Spigler, Stefano},
  journal={Physical Review E},
  volume={95},
  number={2},
  pages={022139},
  year={2017},
  publisher={APS}
}

@article{geiger2019disentangling,
	title = {Disentangling feature and lazy training in deep neural networks},
	volume = {2020},
	issn = {1742-5468},
	url = {https://doi.org/10.1088/1742-5468/abc4de},
	doi = {10.1088/1742-5468/abc4de},
	abstract = {Two distinct limits for deep learning have been derived as the network width h → ∞, depending on how the weights of the last layer scale with h. In the neural tangent Kernel (NTK) limit, the dynamics becomes linear in the weights and is described by a frozen kernel Θ (the NTK). By contrast, in the mean-field limit, the dynamics can be expressed in terms of the distribution of the parameters associated with a neuron, that follows a partial differential equation. In this work we consider deep networks where the weights in the last layer scale as αh −1/2 at initialization. By varying α and h, we probe the crossover between the two limits. We observe two the previously identified regimes of ‘lazy training’ and ‘feature training’. In the lazy-training regime, the dynamics is almost linear and the NTK barely changes after initialization. The feature-training regime includes the mean-field formulation as a limiting case and is characterized by a kernel that evolves in time, and thus learns some features. We perform numerical experiments on MNIST, Fashion-MNIST, EMNIST and CIFAR10 and consider various architectures. We find that: (i) the two regimes are separated by an α* that scales as . (ii) Network architecture and data structure play an important role in determining which regime is better: in our tests, fully-connected networks perform generally better in the lazy-training regime, unlike convolutional networks. (iii) In both regimes, the fluctuations δF induced on the learned function by initial conditions decay as , leading to a performance that increases with h. The same improvement can also be obtained at an intermediate width by ensemble-averaging several networks that are trained independently. (iv) In the feature-training regime we identify a time scale , such that for t ≪ t 1 the dynamics is linear. At t ∼ t 1, the output has grown by a magnitude and the changes of the tangent kernel {\textbar} {\textbar}ΔΘ{\textbar} {\textbar} become significant. Ultimately, it follows for ReLU and Softplus activation functions, with a {\textless} 2 and a → 2 as depth grows. We provide scaling arguments supporting these findings.},
	language = {en},
	number = {11},
	urldate = {2020-12-30},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Geiger, Mario and Spigler, Stefano and Jacot, Arthur and Wyart, Matthieu},
	month = nov,
	year = {2020},
	note = {Publisher: IOP Publishing},
	pages = {113301},
	file = {Submitted Version:C\:\\Users\\leope\\Zotero\\storage\\FCSUFGWL\\Geiger et al. - 2020 - Disentangling feature and lazy training in deep ne.pdf:application/pdf}
}


@article{spigler2019asymptotic,
	title = {Asymptotic learning curves of kernel methods: empirical data versus teacher–student paradigm},
	volume = {2020},
	issn = {1742-5468},
	shorttitle = {Asymptotic learning curves of kernel methods},
	url = {https://doi.org/10.1088/1742-5468/abc61d},
	doi = {10.1088/1742-5468/abc61d},
	abstract = {How many training data are needed to learn a supervised task? It is often observed that the generalization error decreases as n −β where n is the number of training examples and β is an exponent that depends on both data and algorithm. In this work we measure β when applying kernel methods to real datasets. For MNIST we find β ≈ 0.4 and for CIFAR10 β ≈ 0.1, for both regression and classification tasks, and for Gaussian or Laplace kernels. To rationalize the existence of non-trivial exponents that can be independent of the specific kernel used, we study the teacher–student framework for kernels. In this scheme, a teacher generates data according to a Gaussian random field, and a student learns them via kernel regression. With a simplifying assumption—namely that the data are sampled from a regular lattice—we derive analytically β for translation invariant kernels, using previous results from the kriging literature. Provided that the student is not too sensitive to high frequencies, β depends only on the smoothness and dimension of the training data. We confirm numerically that these predictions hold when the training points are sampled at random on a hypersphere. Overall, the test error is found to be controlled by the magnitude of the projection of the true function on the kernel eigenvectors whose rank is larger than n. Using this idea we predict the exponent β from real data by performing kernel PCA, leading to β ≈ 0.36 for MNIST and β ≈ 0.07 for CIFAR10, in good agreement with observations. We argue that these rather large exponents are possible due to the small effective dimension of the data.},
	language = {en},
	number = {12},
	urldate = {2020-12-30},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Spigler, Stefano and Geiger, Mario and Wyart, Matthieu},
	month = dec,
	year = {2020},
	note = {Publisher: IOP Publishing},
	pages = {124001}
}



@InProceedings{Baity18,
  title = 	 {Comparing Dynamics: Deep Neural Networks versus Glassy Systems},
  author = 	 {Baity-Jesi, Marco and Sagun, Levent and Geiger, Mario and Spigler, Stefano and Arous, Gerard Ben and Cammarota, Chiara and LeCun, Yann and Wyart, Matthieu and Biroli, Giulio},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {314--323},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Stockholmsmässan, Stockholm Sweden},
  month = 	 {10--15 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/baity-jesi18a/baity-jesi18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/baity-jesi18a.html},
  abstract = 	 {We analyze numerically the training dynamics of deep neural networks (DNN) by using methods developed in statistical physics of glassy systems. The two main issues we address are the complexity of the loss-landscape and of the dynamics within it, and to what extent DNNs share similarities with glassy systems. Our findings, obtained for different architectures and data-sets, suggest that during the training process the dynamics slows down because of an increasingly large number of flat directions. At large times, when the loss is approaching zero, the system diffuses at the bottom of the landscape. Despite some similarities with the dynamics of mean-field glassy systems, in particular, the absence of barrier crossing, we find distinctive dynamical behaviors in the two cases, thus showing that the statistical properties of the corresponding loss and energy landscapes are different. In contrast, when the network is under-parametrized we observe a typical glassy behavior, thus suggesting the existence of different phases depending on whether the network is under-parametrized or over-parametrized.}
}

@article{Ballard17,
  title={Energy landscapes for machine learning},
  author={Ballard, Andrew J and Das, Ritankar and Martiniani, Stefano and Mehta, Dhagash and Sagun, Levent and Stevenson, Jacob D and Wales, David J},
  journal={Physical Chemistry Chemical Physics},
  year={2017},
  publisher={Royal Society of Chemistry}
}


@article{Sagun16,
  title={Singularity of the Hessian in Deep Learning},
  author={Sagun, Levent and Bottou, {L{\'e}on} and LeCun, Yann},
  journal={International Conference on Learning Representations},
  year={2017}
}

@article{Soudry2016,
  title={No bad local minima: Data independent training error guarantees for multilayer neural networks},
  author={Soudry, Daniel and Carmon, Yair},
  journal={arXiv preprint arXiv:1605.08361},
  year={2016}
}

@inproceedings{Hoffer17,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1729--1739},
  year={2017}
}

@article{Freeman16,
  title={Topology and Geometry of Deep Rectified Network Optimization Landscapes},
  author={Freeman, C Daniel and Bruna, Joan},
  journal={International Conference on Learning Representations},
  year={2017}
}

@article{Hochreiter97,
  title={Flat minima},
  author={Hochreiter, Sepp and Schmidhuber, {J{\"u}rgen}},
  journal={Neural Computation},
  volume={9},
  number={1},
  pages={1--42},
  year={1997},
  publisher={MIT Press}
}

@article{Chaudhari16,
  title={Entropy-sgd: Biasing gradient descent into wide valleys},
  author={Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  journal={arXiv preprint arXiv:1611.01838},
  year={2016}
}

@article{Achille17,
  title={Emergence of invariance and disentangling in deep representations},
  author={Achille, Alessandro and Soatto, Stefano},
  journal={arXiv preprint arXiv:1706.01350},
  year={2017}
}

@article{Zhang16,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={International Conference on Learning Representations},
  year={2017}
}


@inproceedings{Montufar14,
  title={On the number of linear regions of deep neural networks},
  author={Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2924--2932},
  year={2014}
}

@article{Bianchini14,
  title={On the complexity of neural network classifiers: A comparison between shallow and deep architectures},
  author={Bianchini, Monica and Scarselli, Franco},
  journal={IEEE transactions on neural networks and learning systems},
  volume={25},
  number={8},
  pages={1553--1565},
  year={2014},
  publisher={IEEE}
}

@InProceedings{Raghu16,
  title = 	 {On the Expressive Power of Deep Neural Networks},
  author = 	 {Maithra Raghu and Ben Poole and Jon Kleinberg and Surya Ganguli and Jascha Sohl-Dickstein},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2847--2854},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/raghu17a/raghu17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/raghu17a.html},
  abstract = 	 {We propose a new approach to the problem of neural network expressivity, which seeks to characterize how structural properties of a neural network family affect the functions it is able to compute. Our approach is based on an interrelated set of measures of expressivity, unified by the novel notion of trajectory length, which measures how the output of a network changes as the input sweeps along a one-dimensional path. Our findings show that: (1) The complexity of the computed function grows exponentially with depth (2) All weights are not equal: trained networks are more sensitive to their lower (initial) layer weights (3) Trajectory regularization is a simpler alternative to batch normalization, with the same performance.}
}

@inproceedings{Choromanska15,
  title={The loss surfaces of multilayer networks},
  author={Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Ben Arous, G{\'e}rard and LeCun, Yann},
  booktitle={Artificial Intelligence and Statistics},
  pages={192--204},
  year={2015}
}



@inproceedings{Ioffe15,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015}
}

@inproceedings{He16,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{Lecun95,
  title={Convolutional networks for images, speech, and time series},
  author={LeCun, Yann and Bengio, Yoshua and others},
  journal={The handbook of brain theory and neural networks},
  volume={3361},
  number={10},
  pages={1995},
  year={1995}
}

@inproceedings{Krizhevsky12,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@article{Hinton12,
  title={Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups},
  author={Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N and others},
  journal={IEEE Signal processing magazine},
  volume={29},
  number={6},
  pages={82--97},
  year={2012},
  publisher={IEEE}
}

@article{Silver16,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={Nature},
  volume={529},
  number={7587},
  pages={484},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{Silver17,
  title={Mastering the game of Go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={Nature},
  volume={550},
  number={7676},
  pages={354},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{Lecun15,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={Nature},
  volume={521},
  number={7553},
  pages={436},
  year={2015},
  publisher={Nature Publishing Group}
}



@article{Wyart05b,
	Author = {M. Wyart},
	Date-Added = {2014-02-03 23:25:36 +0000},
	Date-Modified = {2014-11-09 08:15:00 +0000},
	Journal = {Annales de Phys},
	Number = {3},
	Pages = {1--113},
	Title = {On the Rigidity of Amorphous Solids},
	Volume = {30},
	Year = {2005}}

@book{Liu10,
author = {J Liu, Andrea and R Nagel, Sidney and Saarloos, W and Wyart, Matthieu},
year = {2010},
month = {06},
pages = {},
title = {The jamming scenario - an introduction and outlook},
booktitle = {Dynamical Heterogeneities in Glasses, Colloids, and Granular Media},
publisher = {OUP Oxford}
}

@article{brito2018theory,
  title={Theory for Swap Acceleration near the Glass and Jamming Transitions},
  author={Brito, Carolina and Lerner, Edan and Wyart, Matthieu},
  journal={arXiv preprint arXiv:1801.03796},
  year={2018}
}

@book{crank1979mathematics,
  title={The mathematics of diffusion},
  author={Crank, John},
  year={1979},
  publisher={Oxford university press}
}

@article{reviewBCKM,
  title={Out of equilibrium dynamics in spin-glasses and other glassy systems},
  author={Bouchaud, Jean-Philippe and Cugliandolo, Leticia F and Kurchan, Jorge and Mezard, Marc},
  journal={Spin glasses and random fields},
  pages={161--223},
  year={1998},
  publisher={World Scientific, Singapore}
}



@incollection{alexnet,
title = {ImageNet Classification with Deep Convolutional Neural Networks},
author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1097--1105},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
}

@incollection{birolileshouches,
  title={Slow relaxations and non-equilibrium dynamics in classical and quantum systems},
  author={Biroli, Giulio},
  editor      = "Thierry Giamarchi, Andrew J. Millis, Olivier Parcollet",
  booktitle   = "Strongly Interacting Quantum Systems Out of Equilibrium",
  publisher   = "Oxford University Press",
  address     = "Oxford",
  year        = 2016,
  pages       = "207-261",
}
@article{benarousj,
  title={Spectral gap estimates in mean field spin glasses},
  author={Ben Arous, {G{\'e}rard} and Jagannath, Aukosh},
  journal={arXiv preprint arXiv:1705.04243},
  year={2017}
}

@article{ninarello2017models,
  title={Models and algorithms for the next generation of glass transition studies},
  author={Ninarello, Andrea and Berthier, Ludovic and Coslovich, Daniele},
  journal={Physical Review X},
  volume={7},
  number={2},
  pages={021039},
  year={2017},
  publisher={APS}
}

@article{mezard2002analytic,
  title={Analytic and algorithmic solution of random satisfiability problems},
  author={M{\'e}zard, Marc and Parisi, Giorgio and Zecchina, Riccardo},
  journal={Science},
  volume={297},
  number={5582},
  pages={812--815},
  year={2002},
  publisher={American Association for the Advancement of Science}
}

@article{monasson1999determining,
  title={Determining computational complexity from characteristic phase transitions},
  author={Monasson, {R{\'e}mi} and Zecchina, Riccardo and Kirkpatrick, Scott and Selman, Bart and Troyansky, Lidror},
  journal={Nature},
  volume={400},
  number={6740},
  pages={133},
  year={1999},
  publisher={Nature Publishing Group}
}

@inproceedings{achlioptas2008algorithmic,
  title={Algorithmic barriers from phase transitions},
  author={Achlioptas, Dimitris and Coja-Oghlan, Amin},
  booktitle={Foundations of Computer Science, 2008. FOCS'08. IEEE 49th Annual IEEE Symposium on},
  pages={793--802},
  year={2008},
  organization={IEEE}
}

@article{zdeborovareview,
  title={Statistical physics of inference: Thresholds and algorithms},
  author={{Zdeborov{\'a}}, Lenka and Krzakala, Florent},
  journal={Advances in Physics},
  volume={65},
  number={5},
  pages={453--552},
  year={2016},
  publisher={Taylor \& Francis}
}

@article{BJREM,
  title={Activated aging dynamics and effective trap model description in the random energy model},
  author={Baity-Jesi, Marco and Biroli, Giulio and Cammarota, Chiara},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2018},
  number={1},
  pages={013301},
  year={2018},
  publisher={IOP Publishing}
}

@article{geiger2019scaling,
	title = {Scaling description of generalization with number of parameters in deep learning},
	volume = {2020},
	issn = {1742-5468},
	url = {https://doi.org/10.1088%2F1742-5468%2Fab633c},
	doi = {10.1088/1742-5468/ab633c},
	abstract = {Supervised deep learning involves the training of neural networks with a large number N of parameters. For large enough N, in the so-called over-parametrized regime, one can essentially fit the training data points. Sparsity-based arguments would suggest that the generalization error increases as N grows past a certain threshold N *. Instead, empirical studies have shown that in the over-parametrized regime, generalization error keeps decreasing with N. We resolve this paradox through a new framework. We rely on the so-called Neural Tangent Kernel, which connects large neural nets to kernel methods, to show that the initialization causes finite-size random fluctuations of the neural net output function f N around its expectation . These affect the generalization error for classification: under natural assumptions, it decays to a plateau value in a power-law fashion ∼N −1/2. This description breaks down at a so-called jamming transition N = N *. At this threshold, we argue that diverges. This result leads to a plausible explanation for the cusp in test error known to occur at N *. Our results are confirmed by extensive empirical observations on the MNIST and CIFAR image datasets. Our analysis finally suggests that, given a computational envelope, the smallest generalization error is obtained using several networks of intermediate sizes, just beyond N *, and averaging their outputs.},
	language = {en},
	number = {2},
	urldate = {2020-09-29},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Geiger, Mario and Jacot, Arthur and Spigler, Stefano and Gabriel, Franck and Sagun, Levent and d'Ascoli, Stéphane and Biroli, Giulio and Hongler, Clément and Wyart, Matthieu},
	month = feb,
	year = {2020},
	note = {Publisher: IOP Publishing},
	pages = {023401},
	file = {IOP Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\CAAC4TDN\\Geiger et al. - 2020 - Scaling description of generalization with number .pdf:application/pdf}
}

@inproceedings{woodworth2020kernel,
	title = {Kernel and {Rich} {Regimes} in {Overparametrized} {Models}},
	url = {http://proceedings.mlr.press/v125/woodworth20a.html},
	abstract = {A recent line of work studies overparametrized neural networks in the “kernel regime,” i.e. when  during training the network behaves as a kernelized linear predictor, and thus, training with grad...},
	language = {en},
	urldate = {2020-12-29},
	booktitle = {Conference on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D. and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
	month = jul,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {3635--3673},
}

@article{ghorbani2020neural,
	title = {When {Do} {Neural} {Networks} {Outperform} {Kernel} {Methods}?},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/a9df2255ad642b923d95503b9a7958d8-Abstract.html},
	language = {en},
	urldate = {2020-12-30},
	journal = {Advances in Neural Information Processing Systems},
	author = {Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
	year = {2020},
	file = {Snapshot:C\:\\Users\\leope\\Zotero\\storage\\NTEC7ZLN\\a9df2255ad642b923d95503b9a7958d8-Abstract.html:text/html}
}


@article{sirignano2020mean,
  title={Mean field analysis of neural networks: A central limit theorem},
  author={Sirignano, Justin and Spiliopoulos, Konstantinos},
  journal={Stochastic Processes and their Applications},
  volume={130},
  number={3},
  pages={1820--1852},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{ghorbani2019limitations,
  title={Limitations of lazy training of two-layers neural network},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9111--9121},
  year={2019}
}
@inproceedings{scholkopf_kernel_1999,
	title = {Kernel principal component analysis},
	abstract = {A new method for performing a nonlinear form of Principal Component Analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear map; for instance the space of all possible d-pixel products in images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.},
	booktitle = {Advances in {Kernel} {Methods} - {Support} {Vector} {Learning}},
	publisher = {MIT Press},
	author = {Scholkopf, Bernhard and Smola, Alexander and Müller, Klaus-Robert},
	year = {1999},
	pages = {327--352},
	file = {Citeseer - Snapshot:C\:\\Users\\leope\\Zotero\\storage\\GFPN86ZX\\summary.html:text/html;Citeseer - Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\7X3FNTFW\\Scholkopf et al. - 1999 - Kernel principal component analysis.pdf:application/pdf}
}
@article{oymak2019generalization,
  title={Generalization guarantees for neural networks via harnessing the low-rank structure of the jacobian},
  author={Oymak, Samet and Fabian, Zalan and Li, Mingchen and Soltanolkotabi, Mahdi},
  journal={arXiv preprint arXiv:1906.05392},
  year={2019}
}
@article{kopitkov2019neural,
	title = {Neural {Spectrum} {Alignment}: {Empirical} {Study}},
	author = {Kopitkov, Dmitry and Indelman, Vadim},
  journal={Artificial {Neural} {Networks} and {Machine} {Learning} – {ICANN} 2020},
  year={2020},
  publisher={Springer International Publishing}
}


@article{bach2017breaking,
  title={Breaking the curse of dimensionality with convex neural networks},
  author={Bach, Francis},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={629--681},
  year={2017},
  publisher={JMLR. org}
}

@inproceedings{bordelon2020spectrum,
	title = {Spectrum {Dependent} {Learning} {Curves} in {Kernel} {Regression} and {Wide} {Neural} {Networks}},
	url = {http://proceedings.mlr.press/v119/bordelon20a.html},
	abstract = {We derive analytical expressions for the generalization performance of kernel regression as a function of the number of training samples using theoretical methods from Gaussian processes and statis...},
	language = {en},
	urldate = {2020-12-30},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Bordelon, Blake and Canatar, Abdulkadir and Pehlevan, Cengiz},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1024--1034},
	file = {Snapshot:C\:\\Users\\leope\\Zotero\\storage\\Z2UDNMTA\\bordelon20a.html:text/html;Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\ENNDME6S\\Bordelon et al. - 2020 - Spectrum Dependent Learning Curves in Kernel Regre.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\IDCPMBSK\\bordelon20a.html:text/html}
}
@article{dacey2000center,
  title={Center surround receptive field structure of cone bipolar cells in primate retina},
  author={Dacey, Dennis and Packer, Orin S and Diller, Lisa and Brainard, David and Peterson, Beth and Lee, Barry},
  journal={Vision research},
  volume={40},
  number={14},
  pages={1801--1811},
  year={2000},
  publisher={Elsevier}
}
@article{mezard2017mean,
  title={Mean-field message-passing equations in the Hopfield model and its generalizations},
  author={M{\'e}zard, Marc},
  journal={Physical Review E},
  volume={95},
  number={2},
  pages={022117},
  year={2017},
  publisher={APS}
}

@article{goldt2019modelling,
	title = {Modeling the {Influence} of {Data} {Structure} on {Learning} in {Neural} {Networks}: {The} {Hidden} {Manifold} {Model}},
	volume = {10},
	shorttitle = {Modeling the {Influence} of {Data} {Structure} on {Learning} in {Neural} {Networks}},
	url = {https://link.aps.org/doi/10.1103/PhysRevX.10.041044},
	doi = {10.1103/PhysRevX.10.041044},
	abstract = {Understanding the reasons for the success of deep neural networks trained using stochastic gradient-based methods is a key open problem for the nascent theory of deep learning. The types of data where these networks are most successful, such as images or sequences of speech, are characterized by intricate correlations. Yet, most theoretical work on neural networks does not explicitly model training data or assumes that elements of each data sample are drawn independently from some factorized probability distribution. These approaches are, thus, by construction blind to the correlation structure of real-world datasets and their impact on learning in neural networks. Here, we introduce a generative model for structured datasets that we call the hidden manifold model. The idea is to construct high-dimensional inputs that lie on a lower-dimensional manifold, with labels that depend only on their position within this manifold, akin to a single-layer decoder or generator in a generative adversarial network. We demonstrate that learning of the hidden manifold model is amenable to an analytical treatment by proving a “Gaussian equivalence property” (GEP), and we use the GEP to show how the dynamics of two-layer neural networks trained using one-pass stochastic gradient descent is captured by a set of integro-differential equations that track the performance of the network at all times. This approach permits us to analyze in detail how a neural network learns functions of increasing complexity during training, how its performance depends on its size, and how it is impacted by parameters such as the learning rate or the dimension of the hidden manifold.},
	number = {4},
	urldate = {2020-12-30},
	journal = {Physical Review X},
	author = {Goldt, Sebastian and Mézard, Marc and Krzakala, Florent and Zdeborová, Lenka},
	month = dec,
	year = {2020},
	note = {Publisher: American Physical Society},
	pages = {041044},
	file = {Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\Q8WILTDG\\Goldt et al. - 2020 - Modeling the Influence of Data Structure on Learni.pdf:application/pdf;APS Snapshot:C\:\\Users\\leope\\Zotero\\storage\\MWCILWIK\\PhysRevX.10.html:text/html}
}
@inproceedings{yehudai2019power,
  title={On the power and limitations of random features for understanding neural networks},
  author={Yehudai, Gilad and Shamir, Ohad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6598--6608},
  year={2019}
}

@inproceedings{shankar2020neural,
	title = {Neural {Kernels} {Without} {Tangents}},
	url = {http://proceedings.mlr.press/v119/shankar20a.html},
	abstract = {We investigate the connections between neural networks and simple building blocks in kernel space. In particular, using well established feature space tools such as direct sum, averaging, and momen...},
	language = {en},
	urldate = {2020-12-29},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Shankar, Vaishaal and Fang, Alex and Guo, Wenshuo and Fridovich-Keil, Sara and Ragan-Kelley, Jonathan and Schmidt, Ludwig and Recht, Benjamin},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {8614--8623},
	file = {Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\FEAJ774U\\Shankar et al. - 2020 - Neural Kernels Without Tangents.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\9HNT3HB8\\shankar20a.html:text/html}
}

@inproceedings{arora2019harnessing,
	title = {Harnessing the {Power} of {Infinitely} {Wide} {Deep} {Nets} on {Small}-data {Tasks}},
	url = {https://openreview.net/forum?id=rkl8sJBYvH},
	abstract = {We verify neural tangent kernel is powerful on small data via experiments on UCI datasets, small CIFAR 10 and low-shot learning on VOC07.},
	language = {en},
	urldate = {2020-12-30},
	author = {Arora, Sanjeev and Du, Simon S. and Li, Zhiyuan and Salakhutdinov, Ruslan and Wang, Ruosong and Yu, Dingli},
	month = sep,
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\SNH768II\\Arora et al. - 2019 - Harnessing the Power of Infinitely Wide Deep Nets .pdf:application/pdf;Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\LPBQRZVJ\\forum.html:text/html},
}


@incollection{arora2019exact,
	title = {On {Exact} {Computation} with an {Infinitely} {Wide} {Neural} {Net}},
	url = {http://papers.nips.cc/paper/9025-on-exact-computation-with-an-infinitely-wide-neural-net.pdf},
	urldate = {2020-09-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {8141--8150},
	file = {NIPS Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\XWQDTKB9\\Arora et al. - 2019 - On Exact Computation with an Infinitely Wide Neura.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\leope\\Zotero\\storage\\I7IJCV83\\9025-on-exact-computation-with-an-infinitely-wide-neural-net.html:text/html}
}


@article{mei2018mean,
	title = {A mean field view of the landscape of two-layer neural networks},
	volume = {115},
	copyright = {Copyright © 2018 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/115/33/E7665},
	doi = {10.1073/pnas.1806579115},
	abstract = {Multilayer neural networks are among the most powerful models in machine learning, yet the fundamental reasons for this success defy mathematical understanding. Learning a neural network requires optimizing a nonconvex high-dimensional objective (risk function), a problem that is usually attacked using stochastic gradient descent (SGD). Does SGD converge to a global optimum of the risk or only to a local optimum? In the former case, does this happen because local minima are absent or because SGD somehow avoids them? In the latter, why do local minima reached by SGD have good generalization properties? In this paper, we consider a simple case, namely two-layer neural networks, and prove that—in a suitable scaling limit—SGD dynamics is captured by a certain nonlinear partial differential equation (PDE) that we call distributional dynamics (DD). We then consider several specific examples and show how DD can be used to prove convergence of SGD to networks with nearly ideal generalization error. This description allows for “averaging out” some of the complexities of the landscape of neural networks and can be used to prove a general convergence result for noisy SGD.},
	language = {en},
	number = {33},
	urldate = {2020-09-29},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
	month = aug,
	year = {2018},
	pmid = {30054315},
	note = {Publisher: National Academy of Sciences
Section: PNAS Plus},
	keywords = {gradient flow, neural networks, partial differential equations, stochastic gradient descent, Wasserstein space},
	pages = {E7665--E7671},
	file = {Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\QKVLBAKV\\Mei et al. - 2018 - A mean field view of the landscape of two-layer ne.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\LQ8XF8DU\\E7665.html:text/html}
}

@article{montanarisemerjian,
  title={Rigorous inequalities between length and time scales in glassy systems},
  author={Montanari, Andrea and Semerjian, Guilhem},
  journal={Journal of statistical physics},
  volume={125},
  number={1},
  pages={23},
  year={2006},
  publisher={Springer}
}

@article{cuku,
  title={Analytical solution of the off-equilibrium dynamics of a long-range spin-glass model},
  author={Cugliandolo, Leticia F and Kurchan, Jorge},
  journal={Physical Review Letters},
  volume={71},
  number={1},
  pages={173},
  year={1993},
  publisher={APS}
}

@incollection{cugliandololeshouches,
  title={Course 7: Dynamics of glassy systems},
  author={Cugliandolo, Leticia F},
  booktitle={Slow Relaxations and nonequilibrium dynamics in condensed matter},
  pages={367--521},
  year={2003},
  publisher={Springer}
}

@book{Mezard87,
  title={Spin glass theory and beyond: An Introduction to the Replica Method and Its Applications},
  author={{M{\'e}zard}, Marc and Parisi, Giorgio and Virasoro, Miguel},
  volume={9},
  year={1987},
  publisher={World Scientific Publishing Company}
}

@article{reviewbray,
  title={Theory of phase-ordering kinetics},
  author={Bray, Alan J},
  journal={Advances in Physics},
  volume={51},
  number={2},
  pages={481--587},
  year={2002},
  publisher={Taylor {\&} Francis}
}

@article{eqBAetal,
  title={Cugliandolo-Kurchan equations for dynamics of Spin-Glasses},
  author={Ben Arous, {G\'erard} and Dembo, Amir and Guionnet, Alice},
  journal={Probability theory and related fields},
  volume={136},
  number={4},
  pages={619--660},
  year={2006},
  publisher={Springer}
}

@article{reviewBB,
  title={Theoretical perspective on the glass transition and amorphous materials},
  author={Berthier, Ludovic and Biroli, Giulio},
  journal={Reviews of Modern Physics},
  volume={83},
  number={2},
  pages={587},
  year={2011},
  publisher={APS}
}

@article{coja2018information,
  title={Information-theoretic thresholds from the cavity method},
  author={Coja-Oghlan, Amin and Krzakala, Florent and Perkins, Will and Zdeborova, Lenka},
  journal={Advances in Mathematics},
  volume={333},
  pages={694--795},
  year={2018},
  publisher={Elsevier}
}

@inproceedings{dauphin2014identifying,
  title={Identifying and attacking the saddle point problem in high-dimensional non-convex optimization},
  author={Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2933--2941},
  year={2014}
}

@article{sagun2014explorations,
  title={Explorations on high dimensional landscapes},
  author={Sagun, Levent and {G\"uney}, V. {U\u{g}ur} and {G{\'{e}}rard} {Ben Arous} and LeCun, Yann},
  journal={International Conference on Learning Representations Workshop Contribution, arXiv:1412.6615},
  year={2015}
}

@article{barbier2017phase,
  title={Phase transitions, optimal errors and optimality of message-passing in generalized linear models},
  author={Barbier, Jean and Krzakala, Florent and Macris, Nicolas and Miolane, L{\'e}o and Zdeborov{\'a}, Lenka},
  journal={arXiv preprint arXiv:1708.03395},
  year={2017}
}


@article{pearlmutter1994fast,
  title={Fast exact multiplication by the Hessian},
  author={Pearlmutter, Barak A},
  journal={Neural computation},
  volume={6},
  number={1},
  pages={147--160},
  year={1994},
  publisher={MIT Press}
}


@inproceedings{watanabe2007almost,
  title={Almost all learning machines are singular},
  author={Watanabe, Sumio},
  booktitle={Foundations of Computational Intelligence, 2007. FOCI 2007. IEEE Symposium on},
  pages={383--388},
  year={2007},
  organization={IEEE}
}

@article{panageas2016gradient,
  title={Gradient descent only converges to minimizers: Non-isolated critical points and invariant regions},
  author={Panageas, Ioannis and Piliouras, Georgios},
  journal={arXiv preprint arXiv:1605.00405},
  year={2016}
}


@article{lee2016gradient,
  title={Gradient descent converges to minimizers},
  author={Lee, Jason D and Simchowitz, Max and Jordan, Michael I and Recht, Benjamin},
  journal={University of California, Berkeley},
  volume={1050},
  pages={16},
  year={2016}
}

@article{hardt2015train,
  title={Train faster, generalize better: Stability of stochastic gradient descent},
  author={Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
  journal={arXiv preprint arXiv:1509.01240},
  year={2015}
}

@article{hochreiter1997flat,
  title={Flat minima},
  author={Hochreiter, Sepp and Schmidhuber, {J{\"u}rgen}},
  journal={Neural Computation},
  volume={9},
  number={1},
  pages={1--42},
  year={1997},
  publisher={MIT Press}
}

@article{keskar2016large,
  title={On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}

@article{nocedal2006numerical,
  title={Numerical Optimization, Second Edition},
  author={Nocedal, Jorge and Wright, Stephen J},
  journal={Numerical optimization},
  pages={497--528},
  year={2006},
  publisher={Springer New York}
}

@incollection{bottou2010large,
  title={Large-scale machine learning with stochastic gradient descent},
  author={Bottou, {L{\'e}on}},
  booktitle={Proceedings of COMPSTAT'2010},
  pages={177--186},
  year={2010},
  publisher={Physica-Verlag HD}
}

@article{mei2016landscape,
  title={The landscape of empirical risk for non-convex losses},
  author={Mei, Song and Bai, Yu and Montanari, Andrea},
  journal={arXiv preprint arXiv:1607.06534},
  year={2016}
}

@article{schaul2013no,
  title={No more pesky learning rates.},
  author={Schaul, Tom and Zhang, Sixin and LeCun, Yann},
  journal={ICML (3)},
  volume={28},
  pages={343--351},
  year={2013}
}

@article{bottou1991stochastic,
  title={Stochastic gradient learning in neural networks},
  author={Bottou, {L{\'e}on}},
  journal={Proceedings of Neuro-{N{\i}mes}},
  volume={91},
  number={8},
  year={1991}
}

@article{bourrely1989parallelization,
  title={Parallelization of a neural network learning algorithm on a hypercube},
  author={Bourrely, J},
  journal={Hypercube and distributed computers. Elsiever Science Publishing},
  year={1989}
}

@article{auffinger2013random,
  title={Random matrices and complexity of spin glasses},
  author={Auffinger, Antonio and Ben Arous, {G{\'e}rard} and {\v{C}}ern{\`y}, Ji{\v{r}}{\'\i}},
  journal={Communications on Pure and Applied Mathematics},
  volume={66},
  number={2},
  pages={165--201},
  year={2013},
  publisher={Wiley Online Library}
}

@article{kurchanlaloux,
  title={Phase space geometry and slow dynamics},
  author={Kurchan, Jorge and Laloux, Laurent},
  journal={Journal of Physics A: Mathematical and General},
  volume={29},
  number={9},
  pages={1929},
  year={1996},
  publisher={IOP Publishing},
}

@article {pnasmontanariksat,
	author = {{Krzaka{\l}a}, Florent and Montanari, Andrea and Ricci-Tersenghi, Federico and Semerjian, Guilhem and {Zdeborov{\'a}}, Lenka},
	title = {Gibbs states and the set of solutions of random constraint satisfaction problems},
	volume = {104},
	number = {25},
	pages = {10318--10323},
	year = {2007},
	doi = {10.1073/pnas.0703685104},
	publisher = {National Academy of Sciences},
	abstract = {An instance of a random constraint satisfaction problem defines a random subset ?? (the set of solutions) of a large product space X N (the set of assignments). We consider two prototypical problem ensembles (random k-satisfiability and q-coloring of random regular graphs) and study the uniform measure with support on S. As the number of constraints per variable increases, this measure first decomposes into an exponential number of pure states ({\textquotedblleft}clusters{\textquotedblright}) and subsequently condensates over the largest such states. Above the condensation point, the mass carried by the n largest states follows a Poisson-Dirichlet process. For typical large instances, the two transitions are sharp. We determine their precise location. Further, we provide a formal definition of each phase transition in terms of different notions of correlation between distinct variables in the problem. The degree of correlation naturally affects the performances of many search/sampling algorithms. Empirical evidence suggests that local Monte Carlo Markov chain strategies are effective up to the clustering phase transition and belief propagation up to the condensation point. Finally, refined message passing techniques (such as survey propagation) may also beat this threshold.},
	issn = {0027-8424},
	journal = {Proceedings of the National Academy of Sciences},
}

@article{cavagnaSGpedestrians,
  title={Spin-glass theory for pedestrians},
  author={Castellani, Tommaso and Cavagna, Andrea},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2005},
  number={05},
  pages={P05012},
  year={2005},
  publisher={IOP Publishing},
}

@article{ballard2017perspective,
  title={Perspective: Energy Landscapes for Machine Learning},
  author={Ballard, Andrew J and Das, Ritankar and Martiniani, Stefano and Mehta, Dhagash and Sagun, Levent and Stevenson, Jacob D and Wales, David J},
  journal={arXiv preprint arXiv:1703.07915},
  year={2017}
}

@article{piela1989multiple,
  title={On the multiple-minima problem in the conformational analysis of molecules: deformation of the potential energy hypersurface by the diffusion equation method},
  author={Piela, Lucjan and Kostrowicki, Jaroslaw and Scheraga, Harold A},
  journal={The Journal of Physical Chemistry},
  volume={93},
  number={8},
  pages={3339--3346},
  year={1989},
  publisher={ACS Publications}
}

@inproceedings{mobahi2015theoretical,
  title={A Theoretical Analysis of Optimization by Gaussian Continuation.},
  author={Mobahi, Hossein and Fisher III, John W},
  booktitle={AAAI},
  pages={1205--1211},
  year={2015},
  organization={Citeseer}
}

@inproceedings{mobahi2015link,
  title={On the link between gaussian homotopy continuation and convex envelopes},
  author={Mobahi, Hossein and Fisher III, John W},
  booktitle={International Workshop on Energy Minimization Methods in Computer Vision and Pattern Recognition},
  pages={43--56},
  year={2015},
  organization={Springer}
}

@article{pardalos1994optimization,
  title={Optimization methods for computing global minima of nonconvex potential energy functions},
  author={Pardalos, Panos M and Shalloway, David and Xue, Guoliang},
  journal={Journal of Global Optimization},
  volume={4},
  number={2},
  pages={117--133},
  year={1994},
  publisher={Springer}
}

@article{bengio2009learning,
  title={Learning deep architectures for AI},
  author={Bengio, Yoshua and others},
  journal={Foundations and trends{\textregistered} in Machine Learning},
  volume={2},
  number={1},
  pages={1--127},
  year={2009},
  publisher={Now Publishers, Inc.}
}

@article{gulcehre2016mollifying,
  title={Mollifying Networks},
  author={Gulcehre, Caglar and Moczulski, Marcin and Visin, Francesco and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1608.04980},
  year={2016}
}

@article{baldassi2016unreasonable,
  title = {Unreasonable effectiveness of learning neural networks: {From} accessible states and robust ensembles to basic algorithmic schemes},
  volume = {113},
  issn = {0027-8424, 1091-6490},
  shorttitle = {Unreasonable effectiveness of learning neural networks},
  doi = {10.1073/pnas.1608103113},
  language = {en},
  number = {48},
  urldate = {2016-11-30},
  journal = {Proceedings of the National Academy of Sciences},
  author = {Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer T. and Ingrosso, Alessandro and Lucibello, Carlo and Saglietti, Luca and Zecchina, Riccardo},
  month = nov,
  year = {2016},
  pages = {E7655--E7662},
}

@inproceedings{aubin2018committee,
  title={The committee machine: Computational to statistical gaps in learning a two-layers neural network},
  author={Aubin, Benjamin and Maillard, Antoine and Krzakala, Florent and Macris, Nicolas and Zdeborov{\'a}, Lenka and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3223--3234},
  year={2018}
}

@inproceedings{gabrie2018entropy,
  title={Entropy and mutual information in models of deep neural networks},
  author={Gabri{\'e}, Marylou and Manoel, Andre and Luneau, Cl{\'e}ment and Macris, Nicolas and Krzakala, Florent and Zdeborov{\'a}, Lenka and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1821--1831},
  year={2018}
}

@article{chaudhari2016entropy,
  title={Entropy-SGD: Biasing Gradient Descent Into Wide Valleys},
  author={Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  journal={arXiv preprint arXiv:1611.01838},
  year={2016}
}

@inproceedings{park2019effect,
	title = {The {Effect} of {Network} {Width} on {Stochastic} {Gradient} {Descent} and {Generalization}: an {Empirical} {Study}},
	shorttitle = {The {Effect} of {Network} {Width} on {Stochastic} {Gradient} {Descent} and {Generalization}},
	url = {http://proceedings.mlr.press/v97/park19b.html},
	abstract = {We investigate how the final parameters found by stochastic gradient descent are influenced by over-parameterization. We generate families of models by increasing the number of channels in a base n...},
	language = {en},
	urldate = {2020-12-30},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Park, Daniel and Sohl-Dickstein, Jascha and Le, Quoc and Smith, Samuel},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {5042--5051},
	file = {Snapshot:C\:\\Users\\leope\\Zotero\\storage\\ZLMSGD5R\\park19b.html:text/html;Full Text PDF:C\:\\Users\\leope\\Zotero\\storage\\652UXKDY\\Park et al. - 2019 - The Effect of Network Width on Stochastic Gradient.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\MGMYLNWN\\park19b.html:text/html}
}
@inproceedings{hazan2016graduated,
  title={On graduated optimization for stochastic non-convex problems},
  author={Hazan, Elad and Levy, Kfir Yehuda and Shalev-Shwartz, Shai},
  booktitle={International Conference on Machine Learning},
  pages={1833--1841},
  year={2016}
}

@article{horn1962eigenvalues,
  title={Eigenvalues of sums of Hermitian matrices},
  author={Horn, Alfred},
  journal={Pacific Journal of Mathematics},
  volume={12},
  number={1},
  pages={225--241},
  year={1962},
  publisher={Mathematical Sciences Publishers}
}

@article{thompson1971eigenvalues,
  title={On the eigenvalues of sums of Hermitian matrices},
  author={Thompson, Robert C and Freede, Linda J},
  journal={Linear Algebra and Its Applications},
  volume={4},
  number={4},
  pages={369--376},
  year={1971},
  publisher={Elsevier}
}

@article{knutson2001honeycombs,
  title={Honeycombs and sums of Hermitian matrices},
  author={Knutson, Allen and Tao, Terence},
  journal={Notices Amer. Math. Soc},
  volume={48},
  number={2},
  year={2001}
}

@article{marvcenko1967distribution,
  title={Distribution of eigenvalues for some sets of random matrices},
  author={{Mar{\v{c}}enko}, Vladimir A and Pastur, Leonid Andreevich},
  journal={Mathematics of the USSR-Sbornik},
  volume={1},
  number={4},
  pages={457},
  year={1967},
  publisher={IOP Publishing}
}

@article{bloemendal2016principal,
  title={On the principal components of sample covariance matrices},
  author={Bloemendal, Alex and Knowles, Antti and Yau, Horng-Tzer and Yin, Jun},
  journal={Probability Theory and Related Fields},
  volume={164},
  number={1-2},
  pages={459--552},
  year={2016},
  publisher={Springer}
}

@article{mobahi2016training,
  title={Training Recurrent Neural Networks by Diffusion},
  author={Mobahi, Hossein},
  journal={arXiv preprint arXiv:1601.04114},
  year={2016}
}

@article{baik2005phase,
  title={Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices},
  author={Baik, Jinho and Ben Arous, {G{\'e}rard} and {P{\'e}ch{\'e}}, Sandrine and others},
  journal={The Annals of Probability},
  volume={33},
  number={5},
  pages={1643--1697},
  year={2005},
  publisher={Institute of Mathematical Statistics}
}

@article{moller1993exact,
  title={Exact Calculation of the Product of the Hessian Matrix of Feed-Forward Network Error Functions and a Vector in 0 (N) Time},
  author={{M{\o}ller}, Martin F},
  journal={DAIMI Report Series},
  volume={22},
  number={432},
  year={1993}
}

@article{hassibi1993second,
  title={Second order derivatives for network pruning: Optimal brain surgeon},
  author={Hassibi, Babak and Stork, David G and others},
  journal={Advances in neural information processing systems},
  pages={164--164},
  year={1993},
  publisher={Morgan Kaufmann Publishers}
}

@article{dinh2017sharp,
  title={Sharp Minima Can Generalize For Deep Nets},
  author={Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1703.04933},
  year={2017}
}

@incollection{lecun90,
title = {Optimal Brain Damage},
author = {LeCun, Yann and John S. Denker and Sara A. Solla},
booktitle = {Advances in Neural Information Processing Systems 2},
editor = {D. S. Touretzky},
pages = {598--605},
year = {1990},
publisher = {Morgan-Kaufmann},
url = {http://papers.nips.cc/paper/250-optimal-brain-damage.pdf}
}

@software{autograd15,
  author = {Dougal Maclaurin and David Duvenaud and Matthew Johnson and Ryan P. Adams},
  title = {Autograd: Reverse-mode differentiation of native {P}ython},
  url = {http://github.com/HIPS/autograd},
  version = {1.1.2},
  year = {2015},
}


@inproceedings{han15a,
  title={Learning both Weights and Connections for Efficient Neural Network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  pages={1135--1143},
  year={2015}
}


@article{han15b,
  title={Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={International Conference on Learning Representations (ICLR)},
  year={2016}
}

@article{wen16,
  author    = {Wei Wen and
               Chunpeng Wu and
               Yandan Wang and
               Yiran Chen and
               Hai Li},
  title     = {Learning Structured Sparsity in Deep Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1608.03665},
  year      = {2016},
  url       = {http://arxiv.org/abs/1608.03665},
  timestamp = {Mon, 30 Jan 2017 17:08:13 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/WenWWCL16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@INPROCEEDINGS{liu2015, 
    author={Baoyuan Liu and Min Wang and H. Foroosh and M. Tappen and M. Penksy}, 
    booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
    title={Sparse Convolutional Neural Networks}, 
    year={2015}, 
    pages={806-814}, 
    keywords={matrix decomposition;matrix multiplication;neural nets;object detection;SCNN model;cascade model;object detection problem;sparse convolutional neural networks;sparse decomposition;sparse fully connected layers;sparse matrix multiplication algorithm;Accuracy;Convolutional codes;Kernel;Matrix decomposition;Neural networks;Redundancy;Sparse matrices},
    doi={10.1109/CVPR.2015.7298681}, 
    ISSN={1063-6919}, 
    month={June},
}
@inproceedings{denton14,
 author = {Denton, Emily and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
 title = {Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation},
 booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems},
 series = {NIPS'14},
 year = {2014},
 location = {Montreal, Canada},
 pages = {1269--1277},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2968826.2968968},
 acmid = {2968968},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 


@article{lecun1998efficient,
  title={Efficient backprop},
  author={LeCun, Yann and Bottou, {L\'eon} and Orr, GB and {M{\"u}ller}, K-R},
  journal={Lecture notes in computer science},
  pages={9--50},
  year={1998},
  publisher={Springer}
}
@inproceedings{li2019towards,
  title={Towards explaining the regularization effect of initial large learning rate in training neural networks},
  author={Li, Yuanzhi and Wei, Colin and Ma, Tengyu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={11674--11685},
  year={2019}
}
@article{shwartz2017opening,
  title={Opening the Black Box of Deep Neural Networks via Information},
  author={Shwartz-Ziv, Ravid and Tishby, Naftali},
  journal={arXiv preprint arXiv:1703.00810},
  year={2017}
}

@article{goyal2017accurate,
  title={Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour},
  author={Goyal, Priya and {Doll{\'a}r}, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}

@inproceedings{martens2010deep,
  title={Deep learning via Hessian-free optimization},
  author={Martens, James},
  booktitle={Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
  pages={735--742},
  year={2010}
}

@article{jastrzkebski2017three,
  title={Three Factors Influencing Minima in SGD},
  author={Jastrzebski, {Stanis{\l}aw} and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
  journal={arXiv preprint arXiv:1711.04623},
  year={2017}
}





@article{sagun2017empirical,
  title={Empirical Analysis of the Hessian of Over-Parametrized Neural Networks},
  author={Sagun, Levent and Evci, Utku and {G\"uney}, V. {U\u{g}ur} and Dauphin, Yann and Bottou, {L\'eon}},
  journal={ICLR 2018 Workshop Contribution, arXiv:1706.04454},
  year={2017}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, {L{\'e}on} and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={IEEE}
}



@inproceedings{kawaguchi2016deep,
  title={Deep learning without poor local minima},
  author={Kawaguchi, Kenji},
  booktitle={Advances in Neural Information Processing Systems},
  pages={586--594},
  year={2016}
}

@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013}
}



@article{krishnan2017neumann,
  title={Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks},
  author={Krishnan, Shankar and Xiao, Ying and Saurous, Rif A},
  journal={arXiv preprint arXiv:1712.03298},
  year={2017}
}

@article{Gastaldi17,
  title={Shake-Shake regularization of 3-branch residual networks},
  author={Gastaldi, Xavier},
  journal={International Conference on Learning Representations},
  year={2017}
}



@InProceedings{lee2017ability,
  title = 	 {On the Ability of Neural Nets to Express Distributions},
  author = 	 {Holden Lee and Rong Ge and Tengyu Ma and Andrej Risteski and Sanjeev Arora},
  booktitle = 	 {Proceedings of the 2017 Conference on Learning Theory},
  pages = 	 {1271--1296},
  year = 	 {2017},
  editor = 	 {Satyen Kale and Ohad Shamir},
  volume = 	 {65},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Amsterdam, Netherlands},
  month = 	 {07--10 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v65/lee17a/lee17a.pdf},
  url = 	 {http://proceedings.mlr.press/v65/lee17a.html},
  abstract = 	 {Deep neural nets have caused a revolution in many classification tasks. A related ongoing revolution?also theoretically not understood?concerns their ability to serve as generative models for complicated types of data such as images and texts. These models are trained using ideas like variational autoencoders and Generative Adversarial Networks. We take a first cut at explaining the expressivity of multilayer nets by giving a sufficient criterion for a function to be approximable by a neural network with $n$ hidden layers. A key ingredient is Barron?s Theorem (Barron, 1993), which gives a Fourier criterion for approximability of a function by a neural network with 1 hidden layer. We show that a composition of $n$ functions which satisfy certain Fourier conditions (?Barron functions?) can be approximated by a $n+1$-layer neural network. For probability distributions, this translates into a criterion for a probability distribution to be approximable in Wasserstein distance?a natural metric on probability distributions?by a neural network applied to a fixed base distribution (e.g., multivariate gaussian). Building up recent lower bound work, we also give an example function that shows that composition of Barron functions is more expressive than Barron functions alone.}
}

@book{engel2001statistical,
  title={Statistical mechanics of learning},
  author={Engel, Andreas and Van den Broeck, Christian},
  year={2001},
  publisher={Cambridge University Press}
}

@incollection{prechelt1998early,
  title={Early stopping-but when?},
  author={Prechelt, Lutz},
  booktitle={Neural Networks: Tricks of the trade},
  pages={55--69},
  year={1998},
  publisher={Springer}
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@inproceedings{caruana2001overfitting,
  title={Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping},
  author={Caruana, Rich and Lawrence, Steve and Giles, C Lee},
  booktitle={Advances in neural information processing systems},
  pages={402--408},
  year={2001}
}

@inproceedings{krogh1992simple,
  title={A simple weight decay can improve generalization},
  author={Krogh, Anders and Hertz, John A},
  booktitle={Advances in neural information processing systems},
  pages={950--957},
  year={1992}
}

@inproceedings{haeffele2017global,
  title={Global optimality in neural network training},
  author={Haeffele, Benjamin D and Vidal, {Ren{\'e}}},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7331--7339},
  year={2017}
}

@article{liao2018dynamics,
  title={The Dynamics of Learning: A Random Matrix Approach},
  author={Liao, Zhenyu and Couillet, Romain},
  journal={arXiv preprint arXiv:1805.11917},
  year={2018}
}


@inproceedings{neyshabur_norm-based_2015,
	title = {Norm-based capacity control in neural networks},
	booktitle = {Conference on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
	year = {2015},
	pages = {1376--1401},
}


@article{neyshabur2017geometry,
  title={Geometry of optimization and implicit regularization in deep learning},
  author={Neyshabur, Behnam and Tomioka, Ryota and Salakhutdinov, Ruslan and Srebro, Nathan},
  journal={arXiv preprint arXiv:1705.03071},
  year={2017}
}

@article{neyshabur2018towards,
  title={Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks},
  author={Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
  journal={arXiv preprint arXiv:1805.12076},
  year={2018}
}

@article{venturi2018neural,
  title={Neural Networks with Finite Intrinsic Dimension have no Spurious Valleys},
  author={Venturi, Luca and Bandeira, Afonso and Bruna, Joan},
  journal={arXiv preprint arXiv:1802.06384},
  year={2018}
}

@article{advani2017high,
	title = {High-dimensional dynamics of generalization error in neural networks},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608020303117},
	doi = {10.1016/j.neunet.2020.08.022},
	abstract = {We perform an analysis of the average generalization dynamics of large neural networks trained using gradient descent. We study the practically-relevant “high-dimensional” regime where the number of free parameters in the network is on the order of or even larger than the number of examples in the dataset. Using random matrix theory and exact solutions in linear models, we derive the generalization error and training error dynamics of learning and analyze how they depend on the dimensionality of data and signal to noise ratio of the learning problem. We find that the dynamics of gradient descent learning naturally protect against overtraining and overfitting in large networks. Overtraining is worst at intermediate network sizes, when the effective number of free parameters equals the number of samples, and thus can be reduced by making a network smaller or larger. Additionally, in the high-dimensional regime, low generalization error requires starting with small initial weights. We then turn to non-linear neural networks, and show that making networks very large does not harm their generalization performance. On the contrary, it can in fact reduce overtraining, even without early stopping or regularization of any sort. We identify two novel phenomena underlying this behavior in overcomplete models: first, there is a frozen subspace of the weights in which no learning occurs under gradient descent; and second, the statistical properties of the high-dimensional regime yield better-conditioned input correlations which protect against overtraining. We demonstrate that standard application of theories such as Rademacher complexity are inaccurate in predicting the generalization performance of deep neural networks, and derive an alternative bound which incorporates the frozen subspace and conditioning effects and qualitatively matches the behavior observed in simulation.},
	language = {en},
	urldate = {2020-09-29},
	journal = {Neural Networks},
	author = {Advani, Madhu S. and Saxe, Andrew M. and Sompolinsky, Haim},
	month = sep,
	year = {2020},
	keywords = {Generalization error, Neural networks, Random matrix theory},
	file = {ScienceDirect Snapshot:C\:\\Users\\leope\\Zotero\\storage\\E87WVE3H\\S0893608020303117.html:text/html;Submitted Version:C\:\\Users\\leope\\Zotero\\storage\\ZAZZFPTJ\\Advani et al. - 2020 - High-dimensional dynamics of generalization error .pdf:application/pdf}
}

@article{monasson1995weight,
  title={Weight space structure and internal representations: a direct approach to learning and generalization in multilayer neural networks},
  author={Monasson, R{\'e}mi and Zecchina, Riccardo},
  journal={Physical review letters},
  volume={75},
  number={12},
  pages={2432},
  year={1995},
  publisher={APS}
}

@article{bansal2018minnorm,
  title={Minnorm training: an algorithm for training overcomplete deep neural networks},
  author={Bansal, Yamini and Advani, Madhu and Cox, David D and Saxe, Andrew M},
  journal={arXiv preprint arXiv:1806.00730},
  year={2018}
}

@inproceedings{lecun1990optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John S and Solla, Sara A},
  booktitle={Advances in neural information processing systems},
  pages={598--605},
  year={1990}
}

@article{matheron1963principles,
  title={Principles of geostatistics},
  author={Matheron, Georges},
  journal={Economic geology},
  volume={58},
  number={8},
  pages={1246--1266},
  year={1963},
  publisher={Society of Economic Geologists}
}

@inproceedings{mahajan2018exploring,
  title={Exploring the limits of weakly supervised pretraining},
  author={Mahajan, Dhruv and Girshick, Ross and Ramanathan, Vignesh and He, Kaiming and Paluri, Manohar and Li, Yixuan and Bharambe, Ashwin and van der Maaten, Laurens},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={181--196},
  year={2018}
}

@article{arpit2017closer,
  title={A closer look at memorization in deep networks},
  author={Arpit, Devansh and {Jastrz{\k{e}}bski}, {Stanis{\l}aw} and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and others},
  journal={arXiv preprint arXiv:1706.05394},
  year={2017}
}

@article{laurent2017multilinear,
  title={The Multilinear Structure of ReLU Networks},
  author={Laurent, Thomas and von Brecht, James},
  journal={arXiv preprint arXiv:1712.10132},
  year={2017}
}

@article{urban2016deep,
  title={Do deep convolutional nets really need to be deep and convolutional?},
  author={Urban, Gregor and Geras, Krzysztof J and Kahou, Samira Ebrahimi and Aslan, Ozlem and Wang, Shengjie and Caruana, Rich and Mohamed, Abdelrahman and Philipose, Matthai and Richardson, Matt},
  journal={arXiv preprint arXiv:1603.05691},
  year={2016}
}


@inproceedings{bos1997dynamics,
  title={Dynamics of training},
  author={{B{\"o}s}, Siegfried and Opper, Manfred},
  booktitle={Advances in Neural Information Processing Systems},
  pages={141--147},
  year={1997}
}

@article{le1991eigenvalues,
  title={Eigenvalues of covariance matrices: Application to neural-network learning},
  author={Le Cun, Yann and Kanter, Ido and Solla, Sara A},
  journal={Physical Review Letters},
  volume={66},
  number={18},
  pages={2396},
  year={1991},
  publisher={APS}
}

@article{sirignano2018mean,
	title = {Mean {Field} {Analysis} of {Neural} {Networks}: {A} {Law} of {Large} {Numbers}},
	volume = {80},
	issn = {0036-1399},
	shorttitle = {Mean {Field} {Analysis} of {Neural} {Networks}},
	url = {https://epubs.siam.org/doi/abs/10.1137/18M1192184},
	doi = {10.1137/18M1192184},
	abstract = {Machine learning, and in particular neural network models, have revolutionized fields such as image, text, and speech recognition. Today, many important real-world applications in these areas are driven by neural networks. There are also growing applications in engineering, robotics, medicine, and finance. Despite their immense success in practice, there is limited mathematical understanding of neural networks. This paper illustrates how neural networks can be studied via stochastic analysis and develops approaches for addressing some of the technical challenges which arise. We analyze one-layer neural networks in the asymptotic regime of simultaneously (a) large network sizes and (b) large numbers of stochastic gradient descent training iterations. We rigorously prove that the empirical distribution of the neural network parameters converges to the solution of a nonlinear partial differential equation. This result can be considered a law of large numbers for neural networks. In addition, a consequence of our analysis is that the trained parameters of the neural network asymptotically become independent, a property which is commonly called “propagation of chaos.”},
	number = {2},
	urldate = {2020-12-30},
	journal = {SIAM Journal on Applied Mathematics},
	author = {Sirignano, Justin and Spiliopoulos, Konstantinos},
	month = jan,
	year = {2020},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {725--752},
	file = {Snapshot:C\:\\Users\\leope\\Zotero\\storage\\YUF9HRL6\\18M1192184.html:text/html;Submitted Version:C\:\\Users\\leope\\Zotero\\storage\\JEB5UDWH\\Sirignano and Spiliopoulos - 2020 - Mean Field Analysis of Neural Networks A Law of L.pdf:application/pdf;Snapshot:C\:\\Users\\leope\\Zotero\\storage\\78H6MFXL\\18M1192184.html:text/html}
}

@article{liang2018just,
  title={Just Interpolate: Kernel" Ridgeless" Regression Can Generalize},
  author={Liang, Tengyuan and Rakhlin, Alexander},
  journal={arXiv preprint arXiv:1808.00387},
  year={2018}
}


@article{soudry2018implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={Journal of Machine Learning Research},
  volume={19},
  number={70},
  year={2018}
}

@article{facco_estimating_2017,
	title = {Estimating the intrinsic dimension of datasets by a minimal neighborhood information},
	volume = {7},
	copyright = {2017 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-017-11873-y},
	doi = {10.1038/s41598-017-11873-y},
	abstract = {Analyzing large volumes of high-dimensional data is an issue of fundamental importance in data science, molecular simulations and beyond. Several approaches work on the assumption that the important content of a dataset belongs to a manifold whose Intrinsic Dimension (ID) is much lower than the crude large number of coordinates. Such manifold is generally twisted and curved; in addition points on it will be non-uniformly distributed: two factors that make the identification of the ID and its exploitation really hard. Here we propose a new ID estimator using only the distance of the first and the second nearest neighbor of each point in the sample. This extreme minimality enables us to reduce the effects of curvature, of density variation, and the resulting computational cost. The ID estimator is theoretically exact in uniformly distributed datasets, and provides consistent measures in general. When used in combination with block analysis, it allows discriminating the relevant dimensions as a function of the block size. This allows estimating the ID even when the data lie on a manifold perturbed by a high-dimensional noise, a situation often encountered in real world data sets. We demonstrate the usefulness of the approach on molecular simulations and image analysis.},
	language = {en},
	number = {1},
	journal = {Scientific Reports},
	author = {Facco, Elena and d’Errico, Maria and Rodriguez, Alex and Laio, Alessandro},
	month = {9},
	year = {2017},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {12140}
}


@inproceedings{tan_efficientnet_2019,
	title = {{EfficientNet}: {Rethinking} {Model} {Scaling} for {Convolutional} {Neural} {Networks}},
	shorttitle = {{EfficientNet}},
	url = {http://proceedings.mlr.press/v97/tan19a.html},
	abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically stud...},
	language = {en},
	urldate = {2021-02-06},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Tan, Mingxing and Le, Quoc},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {6105--6114},
	file = {Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\NKYW7ZUM\\Tan and Le - 2019 - EfficientNet Rethinking Model Scaling for Convolu.pdf:application/pdf;Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\N2PC56XU\\tan19a.html:text/html},
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {He, K. and Zhang, X. and Ren, S. and Sun, J.},
	month = jun,
	year = {2016},
	note = {ISSN: 1063-6919},
	keywords = {CIFAR-10, COCO object detection dataset, COCO segmentation, Complexity theory, deep residual learning, deep residual nets, deeper neural network training, Degradation, ILSVRC \& COCO 2015 competitions, ILSVRC 2015 classification task, image classification, image recognition, Image recognition, Image segmentation, ImageNet dataset, ImageNet localization, ImageNet test set, learning (artificial intelligence), neural nets, Neural networks, object detection, residual function learning, residual nets, Training, VGG nets, visual recognition tasks, Visualization},
	pages = {770--778},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\RIHPA9DB\\He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\lpetrini\\Zotero\\storage\\8JM65QCD\\7780459.html:text/html},
}

@techreport{krizhevsky_learning_2009,
	title = {Learning multiple layers of features from tiny images},
	abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it difficult to learn a good set of filters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is significantly},
	author = {Krizhevsky, Alex},
	year = {2009},
	file = {Citeseer - Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\VH8P8Q54\\summary.html:text/html;Citeseer - Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\88WVCAQC\\Krizhevsky - 2009 - Learning multiple layers of features from tiny ima.pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\MJNNCHWK\\summary.html:text/html},
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	shorttitle = {{ImageNet}},
	doi = {10.1109/CVPR.2009.5206848},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Deng, J. and Dong, W. and Socher, R. and Li, L. and {Kai Li} and {Li Fei-Fei}},
	month = jun,
	year = {2009},
	note = {ISSN: 1063-6919},
	keywords = {computer vision, Explosions, Image databases, image resolution, image retrieval, Image retrieval, ImageNet database, Information retrieval, Internet, large-scale hierarchical image database, large-scale ontology, Large-scale systems, multimedia computing, multimedia data, Multimedia databases, Ontologies, ontologies (artificial intelligence), Robustness, Spine, subtree, trees (mathematics), very large databases, visual databases, wordNet structure},
	pages = {248--255},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\lpetrini\\Zotero\\storage\\55K87UMI\\5206848.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\KHS2X95A\\Deng et al. - 2009 - ImageNet A large-scale hierarchical image databas.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\lpetrini\\Zotero\\storage\\HHPF4RUM\\5206848.html:text/html},
}

@article{xiao_fashion-mnist_2017,
	title = {Fashion-{MNIST}: a {Novel} {Image} {Dataset} for {Benchmarking} {Machine} {Learning} {Algorithms}},
	shorttitle = {Fashion-{MNIST}},
	url = {http://arxiv.org/abs/1708.07747},
	abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
	urldate = {2021-02-06},
	journal = {arXiv:1708.07747 [cs, stat]},
	author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
	month = sep,
	year = {2017},
	note = {arXiv: 1708.07747},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Dataset is freely available at https://github.com/zalandoresearch/fashion-mnist Benchmark is available at http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/},
	file = {arXiv Fulltext PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\W8EVABRI\\Xiao et al. - 2017 - Fashion-MNIST a Novel Image Dataset for Benchmark.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\KE88H72Z\\1708.html:text/html},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {1558-2256},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	month = nov,
	year = {1998},
	note = {Conference Name: Proceedings of the IEEE},
	keywords = {2D shape variability, back-propagation, backpropagation, Character recognition, cheque reading, complex decision surface synthesis, convolution, convolutional neural network character recognizers, document recognition, document recognition systems, Feature extraction, field extraction, gradient based learning technique, gradient-based learning, graph transformer networks, GTN, handwritten character recognition, handwritten digit recognition task, Hidden Markov models, high-dimensional patterns, language modeling, Machine learning, Multi-layer neural network, multilayer neural networks, multilayer perceptrons, multimodule systems, Neural networks, optical character recognition, Optical character recognition software, Optical computing, Pattern recognition, performance measure minimization, Principal component analysis, segmentation recognition},
	pages = {2278--2324},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\7QLLZD6U\\Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\lpetrini\\Zotero\\storage\\Q2WAMXHT\\726791.html:text/html},
}

@article{loshchilov_sgdr_2016,
	title = {{SGDR}: {Stochastic} {Gradient} {Descent} with {Warm} {Restarts}},
	shorttitle = {{SGDR}},
	url = {https://openreview.net/forum?id=Skq89Scxx},
	abstract = {We propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance.},
	language = {en},
	urldate = {2021-02-07},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = nov,
	year = {2016},
	file = {Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\ZH8PN2FR\\Loshchilov and Hutter - 2016 - SGDR Stochastic Gradient Descent with Warm Restar.pdf:application/pdf;Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\84JCV4E5\\forum.html:text/html},
}

@article{lecun_backpropagation_1989,
	title = {Backpropagation {Applied} to {Handwritten} {Zip} {Code} {Recognition}},
	volume = {1},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1989.1.4.541},
	doi = {10.1162/neco.1989.1.4.541},
	abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
	number = {4},
	urldate = {2021-03-24},
	journal = {Neural Computation},
	author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
	month = dec,
	year = {1989},
	pages = {541--551},
	file = {Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\2GLGGJAQ\\LeCun et al. - 1989 - Backpropagation Applied to Handwritten Zip Code Re.pdf:application/pdf;Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\5XVXIK7R\\Backpropagation-Applied-to-Handwritten-Zip-Code.html:text/html},
}


@article{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	journal = {ICLR},
	author = {Simonyan, K. and Zisserman, Andrew},
	year = {2015},
	file = {Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\MQMLMNYZ\\Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf},
}

@inproceedings{sandler_mobilenetv2_2018,
	address = {Salt Lake City, UT},
	title = {{MobileNetV2}: {Inverted} {Residuals} and {Linear} {Bottlenecks}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{MobileNetV2}},
	url = {https://ieeexplore.ieee.org/document/8578572/},
	doi = {10.1109/CVPR.2018.00474},
	abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efﬁcient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3.},
	language = {en},
	urldate = {2021-04-13},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
	month = jun,
	year = {2018},
	pages = {4510--4520},
	file = {Sandler et al. - 2018 - MobileNetV2 Inverted Residuals and Linear Bottlen.pdf:C\:\\Users\\lpetrini\\Zotero\\storage\\C39L2AXV\\Sandler et al. - 2018 - MobileNetV2 Inverted Residuals and Linear Bottlen.pdf:application/pdf},
}

@article{ruderman_pooling_2018,
	title = {Pooling is neither necessary nor sufficient for appropriate deformation stability in {CNNs}},
	url = {http://arxiv.org/abs/1804.04438},
	abstract = {Many of our core assumptions about how neural networks operate remain empirically untested. One common assumption is that convolutional neural networks need to be stable to small translations and deformations to solve image recognition tasks. For many years, this stability was baked into CNN architectures by incorporating interleaved pooling layers. Recently, however, interleaved pooling has largely been abandoned. This raises a number of questions: Are our intuitions about deformation stability right at all? Is it important? Is pooling necessary for deformation invariance? If not, how is deformation invariance achieved in its absence? In this work, we rigorously test these questions, and find that deformation stability in convolutional networks is more nuanced than it first appears: (1) Deformation invariance is not a binary property, but rather that different tasks require different degrees of deformation stability at different layers. (2) Deformation stability is not a fixed property of a network and is heavily adjusted over the course of training, largely through the smoothness of the convolutional filters. (3) Interleaved pooling layers are neither necessary nor sufficient for achieving the optimal form of deformation stability for natural image classification. (4) Pooling confers too much deformation stability for image classification at initialization, and during training, networks have to learn to counteract this inductive bias. Together, these findings provide new insights into the role of interleaved pooling and deformation invariance in CNNs, and demonstrate the importance of rigorous empirical testing of even our most basic assumptions about the working of neural networks.},
	urldate = {2021-05-17},
	journal = {arXiv:1804.04438 [cs, stat]},
	author = {Ruderman, Avraham and Rabinowitz, Neil C. and Morcos, Ari S. and Zoran, Daniel},
	month = may,
	year = {2018},
	note = {arXiv: 1804.04438},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: NIPS 2018 submission},
	file = {arXiv Fulltext PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\GKPXJHGC\\Ruderman et al. - 2018 - Pooling is neither necessary nor sufficient for ap.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\BN7H752U\\1804.html:text/html},
}


@inproceedings{fawzi_manitest_2015,
	address = {Swansea},
	title = {Manitest: {Are} classifiers really invariant?},
	isbn = {978-1-901725-53-7},
	shorttitle = {Manitest},
	url = {http://www.bmva.org/bmvc/2015/papers/paper106/index.html},
	doi = {10.5244/C.29.106},
	language = {en},
	urldate = {2021-05-25},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2015},
	publisher = {British Machine Vision Association},
	author = {Fawzi, Alhussein and Frossard, Pascal},
	year = {2015},
	pages = {106.1--106.13},
	file = {Fawzi and Frossard - 2015 - Manitest Are classifiers really invariant.pdf:C\:\\Users\\lpetrini\\Zotero\\storage\\8ILVDIWI\\Fawzi and Frossard - 2015 - Manitest Are classifiers really invariant.pdf:application/pdf},
}

@inproceedings{kanbak_geometric_2018,
	address = {Salt Lake City, UT},
	title = {Geometric {Robustness} of {Deep} {Networks}: {Analysis} and {Improvement}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {Geometric {Robustness} of {Deep} {Networks}},
	url = {https://ieeexplore.ieee.org/document/8578565/},
	doi = {10.1109/CVPR.2018.00467},
	language = {en},
	urldate = {2021-05-25},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Kanbak, Can and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal},
	month = jun,
	year = {2018},
	pages = {4441--4449},
	file = {Kanbak et al. - 2018 - Geometric Robustness of Deep Networks Analysis an.pdf:C\:\\Users\\lpetrini\\Zotero\\storage\\M53E87MI\\Kanbak et al. - 2018 - Geometric Robustness of Deep Networks Analysis an.pdf:application/pdf},
}

@inproceedings{alcorn_strike_2019,
	address = {Long Beach, CA, USA},
	title = {Strike ({With}) a {Pose}: {Neural} {Networks} {Are} {Easily} {Fooled} by {Strange} {Poses} of {Familiar} {Objects}},
	isbn = {978-1-72813-293-8},
	shorttitle = {Strike ({With}) a {Pose}},
	url = {https://ieeexplore.ieee.org/document/8954212/},
	doi = {10.1109/CVPR.2019.00498},
	abstract = {Despite excellent performance on stationary test sets, deep neural networks (DNNs) can fail to generalize to out-of-distribution (OoD) inputs, including natural, nonadversarial ones, which are common in real-world settings. In this paper, we present a framework for discovering DNN failures that harnesses 3D renderers and 3D models. That is, we estimate the parameters of a 3D renderer that cause a target DNN to misbehave in response to the rendered image. Using our framework and a self-assembled dataset of 3D objects, we investigate the vulnerability of DNNs to OoD poses of well-known objects in ImageNet. For objects that are readily recognized by DNNs in their canonical poses, DNNs incorrectly classify 97\% of their pose space. In addition, DNNs are highly sensitive to slight pose perturbations. Importantly, adversarial poses transfer across models and datasets. We ﬁnd that 99.9\% and 99.4\% of the poses misclassiﬁed by Inception-v3 also transfer to the AlexNet and ResNet-50 image classiﬁers trained on the same ImageNet dataset, respectively, and 75.5\% transfer to the YOLOv3 object detector trained on MS COCO.},
	language = {en},
	urldate = {2021-05-25},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Alcorn, Michael A. and Li, Qi and Gong, Zhitao and Wang, Chengfei and Mai, Long and Ku, Wei-Shinn and Nguyen, Anh},
	month = jun,
	year = {2019},
	pages = {4840--4849},
	file = {Alcorn et al. - 2019 - Strike (With) a Pose Neural Networks Are Easily F.pdf:C\:\\Users\\lpetrini\\Zotero\\storage\\YGIVM9Y6\\Alcorn et al. - 2019 - Strike (With) a Pose Neural Networks Are Easily F.pdf:application/pdf},
}

@inproceedings{alaifari_adef_2018,
	title = {{ADef}: an {Iterative} {Algorithm} to {Construct} {Adversarial} {Deformations}},
	shorttitle = {{ADef}},
	url = {https://openreview.net/forum?id=Hk4dFjR5K7},
	abstract = {We propose a new, efficient algorithm to construct adversarial examples by means of deformations, rather than additive perturbations.},
	language = {en},
	urldate = {2021-05-25},
	author = {Alaifari, Rima and Alberti, Giovanni S. and Gauksson, Tandri},
	month = sep,
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\ZJBVIIFC\\Alaifari et al. - 2018 - ADef an Iterative Algorithm to Construct Adversar.pdf:application/pdf;Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\N9UC8VLL\\forum.html:text/html},
}

@inproceedings{athalye_synthesizing_2018,
	title = {Synthesizing {Robust} {Adversarial} {Examples}},
	url = {http://proceedings.mlr.press/v80/athalye18b.html},
	abstract = {Standard methods for generating adversarial examples for neural networks do not consistently fool neural network classifiers in the physical world due to a combination of viewpoint shifts, camera n...},
	language = {en},
	urldate = {2021-05-25},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Athalye, Anish and Engstrom, Logan and Ilyas, Andrew and Kwok, Kevin},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {284--293},
	file = {Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\Z8PG8CPT\\Athalye et al. - 2018 - Synthesizing Robust Adversarial Examples.pdf:application/pdf;Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\HUM83E2D\\athalye18b.html:text/html},
}

@inproceedings{xiao_spatially_2018,
	title = {Spatially {Transformed} {Adversarial} {Examples}},
	url = {https://openreview.net/forum?id=HyydRMZC-},
	abstract = {We propose a new approach for generating adversarial examples based on spatial transformation, which produces perceptually realistic examples compared to existing attacks.},
	language = {en},
	urldate = {2021-05-25},
	author = {Xiao, Chaowei and Zhu, Jun-Yan and Li, Bo and He, Warren and Liu, Mingyan and Song, Dawn},
	month = feb,
	year = {2018},
	file = {Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\4S4WQPT7\\Xiao et al. - 2018 - Spatially Transformed Adversarial Examples.pdf:application/pdf;Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\QP53SZ2W\\forum.html:text/html},
}

@inproceedings{engstrom_exploring_2019,
	title = {Exploring the {Landscape} of {Spatial} {Robustness}},
	url = {http://proceedings.mlr.press/v97/engstrom19a.html},
	abstract = {The study of adversarial robustness has so far largely focused on perturbations bound in \${\textbackslash}ell\_p\$-norms. However, state-of-the-art models turn out to be also vulnerable to other, more natural class...},
	language = {en},
	urldate = {2021-05-25},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Engstrom, Logan and Tran, Brandon and Tsipras, Dimitris and Schmidt, Ludwig and Madry, Aleksander},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {1802--1811},
	file = {Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\MNFWJIHU\\Engstrom et al. - 2019 - Exploring the Landscape of Spatial Robustness.pdf:application/pdf;Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\NPRFGVPQ\\engstrom19a.html:text/html},
}
@inproceedings{shen_anatomical_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Anatomical {Data} {Augmentation} via {Fluid}-{Based} {Image} {Registration}},
	isbn = {978-3-030-59716-0},
	doi = {10.1007/978-3-030-59716-0_31},
	abstract = {We introduce a fluid-based image augmentation method for medical image analysis. In contrast to existing methods, our framework generates anatomically meaningful images via interpolation from the geodesic subspace underlying given samples. Our approach consists of three steps: 1) given a source image and a set of target images, we construct a geodesic subspace using the Large Deformation Diffeomorphic Metric Mapping (LDDMM) model; 2) we sample transformations from the resulting geodesic subspace; 3) we obtain deformed images and segmentations via interpolation. Experiments on brain (LPBA) and knee (OAI) data illustrate the performance of our approach on two tasks: 1) data augmentation during training and testing for image segmentation; 2) one-shot learning for single atlas image segmentation. We demonstrate that our approach generates anatomically meaningful data and improves performance on these tasks over competing approaches. Code is available at https://github.com/uncbiag/easyreg.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2020},
	publisher = {Springer International Publishing},
	author = {Shen, Zhengyang and Xu, Zhenlin and Olut, Sahin and Niethammer, Marc},
	editor = {Martel, Anne L. and Abolmaesumi, Purang and Stoyanov, Danail and Mateus, Diana and Zuluaga, Maria A. and Zhou, S. Kevin and Racoceanu, Daniel and Joskowicz, Leo},
	year = {2020},
	pages = {318--328},
	file = {Springer Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\Q3GHAZJT\\Shen et al. - 2020 - Anatomical Data Augmentation via Fluid-Based Image.pdf:application/pdf},
}
@article{yang2020feature,
  title={Feature learning in infinite-width neural networks},
  author={Yang, Greg and Hu, Edward J},
  journal={arXiv preprint arXiv:2011.14522},
  year={2020}
}

@article{hauberg_dreaming_2016,
	title = {Dreaming {More} {Data}: {Class}-dependent {Distributions} over {Diffeomorphisms} for {Learned} {Data} {Augmentation}},
	shorttitle = {Dreaming {More} {Data}},
	url = {http://arxiv.org/abs/1510.02795},
	abstract = {Data augmentation is a key element in training high-dimensional models. In this approach, one synthesizes new observations by applying pre-specified transformations to the original training data; e.g.{\textasciitilde}new images are formed by rotating old ones. Current augmentation schemes, however, rely on manual specification of the applied transformations, making data augmentation an implicit form of feature engineering. With an eye towards true end-to-end learning, we suggest learning the applied transformations on a per-class basis. Particularly, we align image pairs within each class under the assumption that the spatial transformation between images belongs to a large class of diffeomorphisms. We then learn a class-specific probabilistic generative models of the transformations in a Riemannian submanifold of the Lie group of diffeomorphisms. We demonstrate significant performance improvements in training deep neural nets over manually-specified augmentation schemes. Our code and augmented datasets are available online.},
	urldate = {2021-10-20},
	journal = {arXiv:1510.02795 [cs]},
	author = {Hauberg, Søren and Freifeld, Oren and Larsen, Anders Boesen Lindbo and Fisher III, John W. and Hansen, Lars Kai},
	month = jun,
	year = {2016},
	note = {arXiv: 1510.02795},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\QUXRSTIJ\\Hauberg et al. - 2016 - Dreaming More Data Class-dependent Distributions .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\Z49XJBYC\\1510.html:text/html},
}


@inproceedings{petrini_relative_2021,
	title = {Relative stability toward diffeomorphisms indicates performance in deep nets},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/497476fe61816251905e8baafdf54c23-Abstract.html},
	urldate = {2022-04-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Petrini, Leonardo and Favero, Alessandro and Geiger, Mario and Wyart, Matthieu},
	year = {2021},
	pages = {8727--8739},
}



@article{chen_atomic_1998,
	title = {Atomic {Decomposition} by {Basis} {Pursuit}},
	volume = {20},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/10.1137/S1064827596304010},
	doi = {10.1137/S1064827596304010},
	abstract = {The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries --- stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), Matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB).

Basis Pursuit (BP) is a principle for decomposing a signal into an "optimal" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP, and BOB, including better sparsity and superresolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, in abstract harmonic analysis, total variation denoising, and multiscale edge denoising.

BP in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver.},
	number = {1},
	urldate = {2022-04-08},
	journal = {SIAM Journal on Scientific Computing},
	author = {Chen, Scott Shaobing and Donoho, David L. and Saunders, Michael A.},
	month = jan,
	year = {1998},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {\${\textbackslash}ell{\textasciicircum}1\$ norm optimization, 41A45, 65D15, 65K05, 94A12, cosine packets, denoising, interior-point methods for linear programming, matching pursuit, multiscale edges, overcomplete signal representation, time-frequency analysis, time-scale analysis, total variation denoising, wavelet packets, wavelets},
	pages = {33--61},
	file = {Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\IYYB3ZWA\\Chen et al. - 1998 - Atomic Decomposition by Basis Pursuit.pdf:application/pdf},
}

@article{bietti_deep_2021,
	title = {Deep {Equals} {Shallow} for {ReLU} {Networks} in {Kernel} {Regimes}},
	url = {http://arxiv.org/abs/2009.14397},
	abstract = {Deep networks are often considered to be more expressive than shallow ones in terms of approximation. Indeed, certain functions can be approximated by deep networks provably more efficiently than by shallow ones, however, no tractable algorithms are known for learning such deep models. Separately, a recent line of work has shown that deep networks trained with gradient descent may behave like (tractable) kernel methods in a certain over-parameterized regime, where the kernel is determined by the architecture and initialization, and this paper focuses on approximation for such kernels. We show that for ReLU activations, the kernels derived from deep fully-connected networks have essentially the same approximation properties as their shallow two-layer counterpart, namely the same eigenvalue decay for the corresponding integral operator. This highlights the limitations of the kernel framework for understanding the benefits of such deep architectures. Our main theoretical result relies on characterizing such eigenvalue decays through differentiability properties of the kernel function, which also easily applies to the study of other kernels defined on the sphere.},
	urldate = {2022-03-03},
	journal = {arXiv:2009.14397 [cs, stat]},
	author = {Bietti, Alberto and Bach, Francis},
	month = aug,
	year = {2021},
	note = {arXiv: 2009.14397},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\TL4V8KMJ\\Bietti and Bach - 2021 - Deep Equals Shallow for ReLU Networks in Kernel Re.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\ABUHBMTZ\\2009.html:text/html},
}


@article{geiger_landscape_2021,
	series = {Landscape and training regimes in deep learning},
	title = {Landscape and training regimes in deep learning},
	volume = {924},
	issn = {0370-1573},
	url = {https://www.sciencedirect.com/science/article/pii/S0370157321001290},
	doi = {10.1016/j.physrep.2021.04.001},
	abstract = {Deep learning algorithms are responsible for a technological revolution in a variety of tasks including image recognition or Go playing. Yet, why they work is not understood. Ultimately, they manage to classify data lying in high dimension – a feat generically impossible due to the geometry of high dimensional space and the associated curse of dimensionality. Understanding what kind of structure, symmetry or invariance makes data such as images learnable is a fundamental challenge. Other puzzles include that (i) learning corresponds to minimizing a loss in high dimension, which is in general not convex and could well get stuck bad minima. (ii) Deep learning predicting power increases with the number of fitting parameters, even in a regime where data are perfectly fitted. In this manuscript, we review recent results elucidating (i, ii) and the perspective they offer on the (still unexplained) curse of dimensionality paradox. We base our theoretical discussion on the (h,α) plane where h controls the number of parameters and α the scale of the output of the network at initialization, and provide new systematic measures of performance in that plane for two common image classification datasets. We argue that different learning regimes can be organized into a phase diagram. A line of critical points sharply delimits an under-parametrized phase from an over-parametrized one. In over-parametrized nets, learning can operate in two regimes separated by a smooth cross-over. At large initialization, it corresponds to a kernel method, whereas for small initializations features can be learnt, together with invariants in the data. We review the properties of these different phases, of the transition separating them and some open questions. Our treatment emphasizes analogies with physical systems, scaling arguments and the development of numerical observables to quantitatively test these results empirically. Practical implications are also discussed, including the benefit of averaging nets with distinct initial weights, or the choice of parameters (h,α) optimizing performance.},
	language = {en},
	urldate = {2022-04-08},
	journal = {Physics Reports},
	author = {Geiger, Mario and Petrini, Leonardo and Wyart, Matthieu},
	month = aug,
	year = {2021},
	keywords = {Curse of dimensionality, Deep learning, Feature learning, Jamming, Lazy training, Loss landscape, Neural networks, Neural tangent kernel},
	pages = {1--18},
}


@article{chen_equivalence_2021,
	title = {On the {Equivalence} between {Neural} {Network} and {Support} {Vector} {Machine}},
	url = {http://arxiv.org/abs/2111.06063},
	abstract = {Recent research shows that the dynamics of an infinitely wide neural network (NN) trained by gradient descent can be characterized by Neural Tangent Kernel (NTK) {\textbackslash}citep\{jacot2018neural\}. Under the squared loss, the infinite-width NN trained by gradient descent with an infinitely small learning rate is equivalent to kernel regression with NTK {\textbackslash}citep\{arora2019exact\}. However, the equivalence is only known for ridge regression currently {\textbackslash}citep\{arora2019harnessing\}, while the equivalence between NN and other kernel machines (KMs), e.g. support vector machine (SVM), remains unknown. Therefore, in this work, we propose to establish the equivalence between NN and SVM, and specifically, the infinitely wide NN trained by soft margin loss and the standard soft margin SVM with NTK trained by subgradient descent. Our main theoretical results include establishing the equivalence between NN and a broad family of \${\textbackslash}ell\_2\$ regularized KMs with finite-width bounds, which cannot be handled by prior work, and showing that every finite-width NN trained by such regularized loss functions is approximately a KM. Furthermore, we demonstrate our theory can enable three practical applications, including (i) {\textbackslash}textit\{non-vacuous\} generalization bound of NN via the corresponding KM; (ii) {\textbackslash}textit\{non-trivial\} robustness certificate for the infinite-width NN (while existing robustness verification methods would provide vacuous bounds); (iii) intrinsically more robust infinite-width NNs than those from previous kernel regression. Our code for the experiments are available at {\textbackslash}url\{https://github.com/leslie-CH/equiv-nn-svm\}.},
	urldate = {2022-04-08},
	journal = {arXiv:2111.06063 [cs, math, stat]},
	author = {Chen, Yilan and Huang, Wei and Nguyen, Lam M. and Weng, Tsui-Wei},
	month = nov,
	year = {2021},
	note = {arXiv: 2111.06063},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
	annote = {Comment: 35th Conference on Neural Information Processing Systems (NeurIPS 2021)},
	file = {arXiv Fulltext PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\9JXK59HN\\Chen et al. - 2021 - On the Equivalence between Neural Network and Supp.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\TGTK3W6C\\2111.html:text/html},
}


@article{bach_learning_2022,
	title = {Learning {Theory} from {First} {Principles}},
	journal = {\href{https://francisbach.com/i-am-writing-a-book/}{In preparation}},
	language = {en},
	year = {2022},
	author = {Bach, Francis},
	url = {https://francisbach.com/i-am-writing-a-book/},

}
.
@article{bronstein_geometric_2021,
	title = {Geometric deep learning: {Grids}, groups, graphs, geodesics, and gauges},
	shorttitle = {Geometric deep learning},
	journal = {arXiv preprint arXiv:2104.13478},
	author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar},
	year = {2021},
}

@book{goodfellow_deep_2016,
	address = {Cambridge, Massachusetts},
	title = {Deep {Learning}},
	isbn = {978-0-262-03561-3},
	abstract = {An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.“Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.”―Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
	language = {English},
	publisher = {The MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	month = nov,
	year = {2016}
}


@article{grigorescu_survey_2020,
	title = {A survey of deep learning techniques for autonomous driving},
	volume = {37},
	issn = {1556-4967},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21918},
	doi = {10.1002/rob.21918},
	abstract = {The last decade witnessed increasingly rapid progress in self-driving vehicle technology, mainly backed up by advances in the area of deep learning and artificial intelligence (AI). The objective of this paper is to survey the current state-of-the-art on deep learning technologies used in autonomous driving. We start by presenting AI-based self-driving architectures, convolutional and recurrent neural networks, as well as the deep reinforcement learning paradigm. These methodologies form a base for the surveyed driving scene perception, path planning, behavior arbitration, and motion control algorithms. We investigate both the modular perception-planning-action pipeline, where each module is built using deep learning methods, as well as End2End systems, which directly map sensory information to steering commands. Additionally, we tackle current challenges encountered in designing AI architectures for autonomous driving, such as their safety, training data sources, and computational hardware. The comparison presented in this survey helps gain insight into the strengths and limitations of deep learning and AI approaches for autonomous driving and assist with design choices.},
	language = {en},
	number = {3},
	urldate = {2022-04-25},
	journal = {Journal of Field Robotics},
	author = {Grigorescu, Sorin and Trasnea, Bogdan and Cocias, Tiberiu and Macesanu, Gigel},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.21918},
	keywords = {AI for self-driving vehicles, artificial intelligence, autonomous driving, deep learning for autonomous driving},
	pages = {362--386},
	file = {Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\9PJE9HGI\\Grigorescu et al. - 2020 - A survey of deep learning techniques for autonomou.pdf:application/pdf;Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\IZ8QWQNP\\rob.html:text/html},
}


@inproceedings{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
	abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
	urldate = {2022-04-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year = {2020},
	pages = {1877--1901},
	file = {Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\KFSWPXG2\\Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf},
}

@inproceedings{favero_locality_2021,
	title = {Locality defeats the curse of dimensionality in convolutional teacher-student scenarios},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/4e8eaf897c638d519710b1691121f8cb-Abstract.html},
	urldate = {2022-04-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Favero, Alessandro and Cagnetta, Francesco and Wyart, Matthieu},
	year = {2021},
	pages = {9456--9467},
	file = {Full Text PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\7WAEKKRU\\Favero et al. - 2021 - Locality defeats the curse of dimensionality in co.pdf:application/pdf},
}


@article{basri_convergence_2019,
	title = {The {Convergence} {Rate} of {Neural} {Networks} for {Learned} {Functions} of {Different} {Frequencies}},
	url = {http://arxiv.org/abs/1906.00425},
	abstract = {We study the relationship between the frequency of a function and the speed at which a neural network learns it. We build on recent results that show that the dynamics of overparameterized neural networks trained with gradient descent can be well approximated by a linear system. When normalized training data is uniformly distributed on a hypersphere, the eigenfunctions of this linear system are spherical harmonic functions. We derive the corresponding eigenvalues for each frequency after introducing a bias term in the model. This bias term had been omitted from the linear network model without significantly affecting previous theoretical results. However, we show theoretically and experimentally that a shallow neural network without bias cannot represent or learn simple, low frequency functions with odd frequencies. Our results lead to specific predictions of the time it will take a network to learn functions of varying frequency. These predictions match the empirical behavior of both shallow and deep networks.},
	urldate = {2022-03-03},
	journal = {arXiv:1906.00425 [cs, eess, stat]},
	author = {Basri, Ronen and Jacobs, David and Kasten, Yoni and Kritchman, Shira},
	month = dec,
	year = {2019},
	note = {arXiv: 1906.00425},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Electrical Engineering and Systems Science - Signal Processing},
	annote = {relu has only odd components on spherical harmonics, when adding a bias all components are restored.},
	file = {arXiv Fulltext PDF:C\:\\Users\\lpetrini\\Zotero\\storage\\NMRIIC9I\\Basri et al. - 2019 - The Convergence Rate of Neural Networks for Learne.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\lpetrini\\Zotero\\storage\\U8MLFK3T\\1906.html:text/html},
}


@article{cui_generalization_2021,
	title = {Generalization error rates in kernel regression: {The} crossover from the noiseless to noisy regime},
	volume = {34},
	shorttitle = {Generalization error rates in kernel regression},
	journal = {Advances in Neural Information Processing Systems},
	author = {Cui, Hugo and Loureiro, Bruno and Krzakala, Florent and Zdeborová, Lenka},
	year = {2021},
}
